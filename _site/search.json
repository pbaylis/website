[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "ECON 371: Economics of the environment \\(\\cdot\\) Syllabus\n\nRegistration note: Sadly, this course is often full and I am unable to make individual exceptions to add students. If this a required course for graduation please get in touch with the Economics Undergraduate Administrative Assistant (contact). Otherwise, some drops do occur early in the term so please keep an eye on registration.\n\nECON 494: Seminar in environmental economics \\(\\cdot\\) Syllabus\nECON 573: Graduate environmental economics \\(\\cdot\\) Syllabus"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Mandated vs. Voluntary Adaptation to Natural Disasters: The Case of U.S. Wildfires (with Judson Boomhower, accepted at the Journal of Political Economy)\n\nNBER Working Paper (December 2021)\nSelected media: Headwaters Economics Blog Post \\(\\cdot\\) New York Times \\(\\cdot\\) Marketplace \\(\\cdot\\)The Weeds \\(\\cdot\\) Marketwatch \\(\\cdot\\) LA Times\n\nQuantifying Fire-Specific Smoke Severity (with Jeff Wen, Marshall Burke, and Judson Boomhower, Proceedings of the National Academy of Sciences, 2023)\n\nPaper \\(\\cdot\\) Preprint\nSelected media: New York Times 1 \\(\\cdot\\) New York Times 2\n\nThe Economic Incidence of Wildfire Suppression in the United States (with Judson Boomhower, previously titled “Moral Hazard, Wildfires, and the Economic Incidence of Natural Disasters,” AEJ: Applied Economics, 2023)\n\nPaper \\(\\cdot\\) NBER Working Paper\n\nExposures and Behavioral Responses to Wildfire Smoke (with Marshall Burke, Sam Heft-Neal, Jessica Li, Anne Driscoll, Matthieu Stigler, Joakim Weill, Jennifer Burney, Jeff Wen, Marissa Childs, and Carlos Gould, Nature: Human Behavior, 2022)\n\nPaper \\(\\cdot\\) NBER Working Paper\n\nDefault Effects, Follow-on Behavior and Welfare in Residential Electricity Pricing Program (with Meredith Fowlie, Catherine Wolfram, C. Anna Spurlock, Annika Todd, and Peter Cappers, The Review of Economic Studies, 2021)\n\nPaper \\(\\cdot\\) NBER Working Paper\n\nThe Distribution of COVID-19 Related Risks (with Pierre-Loup Beauregard, Marie Connolly, Nicole Fortin, David A. Green, Pablo Gutierrez Cubillos, Samuel S. Gyetvay, Catherine Haeck, Timea Laura Molnar, Gaëlle Simard-Duplain, Henry E. Siu, Maria teNyenhuis, and Casey Warman, Canadian Journal of Economics, 2021)\n\nPaper \\(\\cdot\\) NBER Working Paper \\(\\cdot\\) CIRANO Working Paper\nVSE COVID Risk/Reward Assessment Tool \\(\\cdot\\) Repo\n\nJob Loss and Behavioral Change: The Unprecedented Effects of the India Lockdown in Delhi (with Kenneth Lee, Harshil Sahai, and Michael Greenstone, COVID Economics, 2021)\n\nPaper \\(\\cdot\\) SSRN Working Paper\n\nTemperature and Temperament: Evidence from Twitter (Journal of Public Economics, 2020)\n\nPaper \\(\\cdot\\) Pre-print\nSelected media: Popular Science \\(\\cdot\\) Washington Post \\(\\cdot\\) Time \\(\\cdot\\) Weather Channel \\(\\cdot\\) UC Newsroom \\(\\cdot\\) Energy Institute \\(\\cdot\\) G-FEED \\(\\cdot\\) Marginal Revolution\n\nRapidly Declining Remarkability of Temperature Anomalies May Obscure Public Perception of Climate Change (with Frances C. Moore, Nick Obradovich, and Flavio Lehner, Proceedings of the National Academy of Sciences, 2019)\n\nPaper\nSelected media: New York Times \\(\\cdot\\) Washington Post (op-ed by Moore and Obradovich)\n\nHigher Temperatures Increase Suicide Rates in the United States and Mexico (with Marshall Burke, Felipe González, Sam Heft-Neal, Ceren Baysan, Sanjay Basu, and Solomon Hsiang, Nature Climate Change, 2018)\n\nPaper \\(\\cdot\\) Comment \\(\\cdot\\) Response\nSelected media: The Atlantic \\(\\cdot\\) The Guardian \\(\\cdot\\) Reuters\n\nWeather Impacts Expressed Sentiment (with Nick Obradovich, Yury Kryvasheyeu, Haohui Chen, Lorenzo Coviello, Esteban Moro, Manuel Cebrian, and James H. Fowler, PLOS ONE, 2018)\n\nPaper\nSelected media: MIT Technology Review\n\nClimate Change is Projected to Have Severe Impacts on the Frequency and Intensity of Peak Electricity Demand Across the United States (with Max Auffhammer and Catherine H. Hausman, Proceedings of the National Academy of Sciences, 2017)\n\nPaper\nSelected media: New York Times \\(\\cdot\\) ARS Technica"
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Research",
    "section": "",
    "text": "Mandated vs. Voluntary Adaptation to Natural Disasters: The Case of U.S. Wildfires (with Judson Boomhower, accepted at the Journal of Political Economy)\n\nNBER Working Paper (December 2021)\nSelected media: Headwaters Economics Blog Post \\(\\cdot\\) New York Times \\(\\cdot\\) Marketplace \\(\\cdot\\)The Weeds \\(\\cdot\\) Marketwatch \\(\\cdot\\) LA Times\n\nQuantifying Fire-Specific Smoke Severity (with Jeff Wen, Marshall Burke, and Judson Boomhower, Proceedings of the National Academy of Sciences, 2023)\n\nPaper \\(\\cdot\\) Preprint\nSelected media: New York Times 1 \\(\\cdot\\) New York Times 2\n\nThe Economic Incidence of Wildfire Suppression in the United States (with Judson Boomhower, previously titled “Moral Hazard, Wildfires, and the Economic Incidence of Natural Disasters,” AEJ: Applied Economics, 2023)\n\nPaper \\(\\cdot\\) NBER Working Paper\n\nExposures and Behavioral Responses to Wildfire Smoke (with Marshall Burke, Sam Heft-Neal, Jessica Li, Anne Driscoll, Matthieu Stigler, Joakim Weill, Jennifer Burney, Jeff Wen, Marissa Childs, and Carlos Gould, Nature: Human Behavior, 2022)\n\nPaper \\(\\cdot\\) NBER Working Paper\n\nDefault Effects, Follow-on Behavior and Welfare in Residential Electricity Pricing Program (with Meredith Fowlie, Catherine Wolfram, C. Anna Spurlock, Annika Todd, and Peter Cappers, The Review of Economic Studies, 2021)\n\nPaper \\(\\cdot\\) NBER Working Paper\n\nThe Distribution of COVID-19 Related Risks (with Pierre-Loup Beauregard, Marie Connolly, Nicole Fortin, David A. Green, Pablo Gutierrez Cubillos, Samuel S. Gyetvay, Catherine Haeck, Timea Laura Molnar, Gaëlle Simard-Duplain, Henry E. Siu, Maria teNyenhuis, and Casey Warman, Canadian Journal of Economics, 2021)\n\nPaper \\(\\cdot\\) NBER Working Paper \\(\\cdot\\) CIRANO Working Paper\nVSE COVID Risk/Reward Assessment Tool \\(\\cdot\\) Repo\n\nJob Loss and Behavioral Change: The Unprecedented Effects of the India Lockdown in Delhi (with Kenneth Lee, Harshil Sahai, and Michael Greenstone, COVID Economics, 2021)\n\nPaper \\(\\cdot\\) SSRN Working Paper\n\nTemperature and Temperament: Evidence from Twitter (Journal of Public Economics, 2020)\n\nPaper \\(\\cdot\\) Pre-print\nSelected media: Popular Science \\(\\cdot\\) Washington Post \\(\\cdot\\) Time \\(\\cdot\\) Weather Channel \\(\\cdot\\) UC Newsroom \\(\\cdot\\) Energy Institute \\(\\cdot\\) G-FEED \\(\\cdot\\) Marginal Revolution\n\nRapidly Declining Remarkability of Temperature Anomalies May Obscure Public Perception of Climate Change (with Frances C. Moore, Nick Obradovich, and Flavio Lehner, Proceedings of the National Academy of Sciences, 2019)\n\nPaper\nSelected media: New York Times \\(\\cdot\\) Washington Post (op-ed by Moore and Obradovich)\n\nHigher Temperatures Increase Suicide Rates in the United States and Mexico (with Marshall Burke, Felipe González, Sam Heft-Neal, Ceren Baysan, Sanjay Basu, and Solomon Hsiang, Nature Climate Change, 2018)\n\nPaper \\(\\cdot\\) Comment \\(\\cdot\\) Response\nSelected media: The Atlantic \\(\\cdot\\) The Guardian \\(\\cdot\\) Reuters\n\nWeather Impacts Expressed Sentiment (with Nick Obradovich, Yury Kryvasheyeu, Haohui Chen, Lorenzo Coviello, Esteban Moro, Manuel Cebrian, and James H. Fowler, PLOS ONE, 2018)\n\nPaper\nSelected media: MIT Technology Review\n\nClimate Change is Projected to Have Severe Impacts on the Frequency and Intensity of Peak Electricity Demand Across the United States (with Max Auffhammer and Catherine H. Hausman, Proceedings of the National Academy of Sciences, 2017)\n\nPaper\nSelected media: New York Times \\(\\cdot\\) ARS Technica"
  },
  {
    "objectID": "posts/2020-02-28-specification-curve/2020-02-28-specification-curve.html",
    "href": "posts/2020-02-28-specification-curve/2020-02-28-specification-curve.html",
    "title": "How to plot a specification curve",
    "section": "",
    "text": "Like many researchers, I often want to plot a range of coefficient estimates to figure out if the results I’m finding are robust to other sensible specification and functional form choices. This kind of estimate is called a sensitivity curve (Simonsohn, Simmons, and Nelson 2015), and I am far from the first to do it. In fact, there are even a couple packages available: Joachim Gassen’s rdfanalysis and Philipp Masur’s specr (I haven’t used either, yet).\nI wanted to roll my own, though. Since my code is fairly simple (less than 60 lines including comments and generous spacing) and uses the tidyverse, it may be helpful to other people too. First, here’s the output. It’s certainly not the prettiest version of a specification curve, but it gets the job done.\n\n\n\nThis is my specification curve. There are many like it, but this one is mine.\n\n\nSecond, here’s the code (also available as a gist) that creates it.\n# Create sensitivity curve of coefficient estimates\nlibrary(tidyverse)\nlibrary(cowplot)\nlibrary(fastDummies)\n\n# Setup ----\nrm(list = ls())\ntheme_set(theme_cowplot())\nset.seed(42)\n\n# Create some fake estimates to plot ----\n# Required components: est, se, and any variables that describe specifications\n\n# These are the possible specifications we \"estimated\"\nfe &lt;- c(\"Unit\", \"Time\", \"Unit + Time\")\ncontrols &lt;- c(\"Basic\", \"Some\", \"Full\")\n\nestimates &lt;- as_tibble(expand.grid(`Fixed Effects` = fe, `Controls` = controls))\nestimates &lt;- estimates %&gt;% mutate(est = rnorm(n()), se = runif(n(), 0, 0.1))\n\n# Create a plot of the estimates ----\nspec_cols &lt;- c(\"Fixed Effects\", \"Controls\") # Could be set by user or figured out\n# Note: This assumes the preferred ordering of the specification categories is the order in which they are given\n\nestimates &lt;- estimates %&gt;% mutate(ci_l = est - 1.96 * se, ci_h = est + 1.96 * se)\n\nestimates &lt;- estimates %&gt;% \n    arrange(est) %&gt;% mutate(h_order = 1:n()) # Sort on point estimates for horizontal ordering\n\ncoef_plot &lt;- ggplot(estimates, aes(x = h_order, y = est)) + \n    geom_linerange(aes(ymin = ci_l, ymax = ci_h), size = 1, alpha = 0.5) + \n    geom_point(fill = \"white\", shape = 21) + \n    labs(y = \"Coefficient\") + \n    theme(axis.title.x = element_blank(), axis.ticks.x = element_blank(), axis.line.x = element_blank(), axis.text.x = element_blank())\ncoef_plot\n\n# Create a plot of the specifications ----\n\n# Function to create a specification plot for a single category. \nmake_spec_plot &lt;- function(category) {\n    # category = spec_cols[1] # DEBUG\n    specs &lt;- dummy_cols(estimates, select_columns = category, remove_selected_columns = T) %&gt;%\n        select(h_order, starts_with(category)) %&gt;% \n        pivot_longer(starts_with(category), names_prefix = paste0(category, \"_\")) %&gt;%\n        mutate(name = factor(name, levels = rev(unique(name)))) \n    \n    spec_plot &lt;- ggplot(specs, aes(x = h_order, y = name, alpha = value)) +\n        geom_point() + \n        scale_alpha_continuous(guide = FALSE) +\n        theme(axis.title.x = element_blank(), axis.ticks.x = element_blank(), axis.line.x = element_blank(), axis.text.x = element_blank()) + \n        theme(axis.text.y = element_text(size = 6), axis.title.y = element_blank(), axis.ticks.y = element_blank(), axis.line.y = element_blank())\n    spec_plot    \n}\nspec_plots &lt;- lapply(spec_cols, make_spec_plot)\n\ncombined_plot &lt;- plot_grid(plotlist = c(list(coef_plot), spec_plots), \n          labels = c(\"\", spec_cols), label_size = 8, label_fontface = \"italic\", vjust = 0.5, hjust = -0.1,\n          rel_heights = c(1, 0.2, 0.2), ncol = 1, align = \"v\")\nsave_plot(\"sensitivity-curve.png\", combined_plot)"
  },
  {
    "objectID": "posts/2017-02-03-r-speed-tests/2017-02-03-r-speed-tests.html",
    "href": "posts/2017-02-03-r-speed-tests/2017-02-03-r-speed-tests.html",
    "title": "R performance tests",
    "section": "",
    "text": "Warning\n\n\n\nUpdate (July 2020): These tests are quite dated at this point, YMMV. Software changes quickly!"
  },
  {
    "objectID": "posts/2017-02-03-r-speed-tests/2017-02-03-r-speed-tests.html#r-date-conversion-speed-test-as.idate-vs-fast_strptime",
    "href": "posts/2017-02-03-r-speed-tests/2017-02-03-r-speed-tests.html#r-date-conversion-speed-test-as.idate-vs-fast_strptime",
    "title": "R performance tests",
    "section": "R date conversion speed test (as.IDate vs fast_strptime)",
    "text": "R date conversion speed test (as.IDate vs fast_strptime)\nrequire(data.table)\nrequire(lubridate)\n\nn &lt;- 10000000\nx &lt;- rep(\"2014-01-01\", n)\n\nsystem.time(r1 &lt;- as.IDate(x, format=\"%Y%m%d\"))\nsystem.time(r2 &lt;- as.IDate(parse_date_time(x, orders=\"%Y%m%d\", exact=T)))\nsystem.time(r3 &lt;- as.IDate(fast_strptime(x, format=\"%Y%m%d\")))\nWinner: fast_strptime by a factor of two over the IDate parser (which is also the Date parser?)."
  },
  {
    "objectID": "posts/2017-02-03-r-speed-tests/2017-02-03-r-speed-tests.html#pattern-matching-grep-vs-str",
    "href": "posts/2017-02-03-r-speed-tests/2017-02-03-r-speed-tests.html#pattern-matching-grep-vs-str",
    "title": "R performance tests",
    "section": "Pattern matching (grep vs str)",
    "text": "Pattern matching (grep vs str)\nWhat’s the fastest way to match strings? This code compares grep to stri_detect_* (from the stringi package), considering both fixed and regex matching.\nlibrary(microbenchmark)\nlibrary(stringi)\nlibrary(ggplot2)\n\nR &lt;- 100000\ng &lt;- replicate(R, paste0(sample(c(letters[1:5],\" \"), 10, replace=TRUE),\n                               collapse=\"\"))\n\nm &lt;- microbenchmark(\n  grep(\" \", g ),\n  stri_detect_regex(g, \" \"),\n  grep(\" \", g, perl=TRUE),\n  grep(\" \", g, fixed=TRUE),\n  stri_detect_fixed(g, \" \")\n)\nautoplot(m)\n\n\n\npng\n\n\nResults are similar for gsub. For a comparison of stringi to stringr, see here.\nSee also here for improving grep performance.\n\nRead CSV (fread vs read_csv)\nI use fread (from the data.table package) for my day-to-day data munging in R, but occasionally read_csv (from the readr package) is more useful, for example when CSVs are formatted in a tricky way or when I’d prefer to have dates read in automatically. It’s helpful to know what kind of performance tradeoff I’m making. Following code tests timings on reading both character and numeric vectors. Timings in comments in seconds.\nlibrary(data.table)\nlibrary(readr)\nlibrary(stringi)\n\n# Create test dataframes\nn &lt;- 10000000\ndf1 &lt;- data.frame(x=stri_rand_strings(n, 5, '[A-Z]'))\ndf1$x &lt;-as.character(df1$x)\ndf2 &lt;- data.frame(x=round(rnorm(n), 3))\n\ndt1 &lt;- data.table(df1)\ndt2 &lt;- data.table(df2)\n\nsystem.time(write_csv(df1, \"dt1_df.csv\")) # 3.8\nsystem.time(write_csv(df2, \"dt2_df.csv\")) # 3.1\nsystem.time(fwrite(dt1, \"dt1_dt.csv\")) # 0.6\nsystem.time(fwrite(dt2, \"dt2_dt.csv\")) # 1.3\n\nsystem.time(in.df1 &lt;- read_csv(\"dt1_df.csv\")) # 4.9\nsystem.time(in.df2 &lt;- read_csv(\"dt2_df.csv\")) # 2.2\nsystem.time(in.dt1 &lt;- fread(\"dt1_dt.csv\")) # 2.7\nsystem.time(in.dt2 &lt;- fread(\"dt2_dt.csv\")) # 1.0\nSo data.table is about three times as fast at writing and two times at fast at reading."
  },
  {
    "objectID": "posts/2017-02-03-r-speed-tests/2017-02-03-r-speed-tests.html#write-csv-fwrite-vs-write_csv",
    "href": "posts/2017-02-03-r-speed-tests/2017-02-03-r-speed-tests.html#write-csv-fwrite-vs-write_csv",
    "title": "R performance tests",
    "section": "Write CSV (fwrite vs write_csv)",
    "text": "Write CSV (fwrite vs write_csv)\nUnsupported anecdotal claim: fwrite is faster."
  },
  {
    "objectID": "posts/2018-10-11-beamer-resizing/2018-10-11-beamer-resizing.html",
    "href": "posts/2018-10-11-beamer-resizing/2018-10-11-beamer-resizing.html",
    "title": "Automatically size beamer images and tables",
    "section": "",
    "text": "“Democracy is the worst form of government, except for all the others” - Winston Churchill (although apparently he was quoting someone else)\nThis is how I feel about beamer. I still haven’t figured out the perfect solution. Markdown + pandoc can work, but I always end up hacking in LaTeX, since I can be particular about what my slides look like. Recently I’ve started using the metropolis theme, which does a lot of things better than vanilla beamer. It doesn’t avoid all of the usual beamer pain, though. My least favorite (and most repetitive) activity is resizing figures and tables to fit slides properly. In preparation for an upcoming presentation, I decided to see if there was a way around that. There is. Below is some code that autoscales images and tables (with notes) in a beamer presentation.\n\nAutoscaled beamer presentation template\n\\documentclass{beamer}\n\n\\usetheme{metropolis} % Preferred beamer theme\n\n\\usepackage{graphicx} % Load graphics\n\\usepackage{booktabs} % Nice tables\n\\usepackage{dcolumn} % Booktabs column spacing\n\\usepackage{threeparttable} % Align column caption, table, and notes\n\\usepackage{adjustbox} % Shrink stuff\n\\usepackage{showframe} % Useful for debugging\n\n% Fancy fit image command with optional caption\n\\makeatletter\n\\newcommand{\\fitimage}[2][\\@nil]{\n  \\begin{figure}\n    \\begin{adjustbox}{width=0.9\\textwidth, totalheight=\\textheight-2\\baselineskip-2\\baselineskip,keepaspectratio}\n      \\includegraphics{#2}\n    \\end{adjustbox}\n    \\def\\tmp{#1}%\n   \\ifx\\tmp\\@nnil\n      \\else\n      \\caption{#1}\n    \\fi\n  \\end{figure}\n}\n\\makeatother\n\n\\begin{document}\n\n\\begin{frame}{Autoscaling image}\n  \\fitimage[Autoscaling image]{myimage.png}\n\\end{frame}\n\n\\begin{frame}{Autoscaling table}\n        \\begin{table}\n            \\begin{adjustbox}{width=\\textwidth, totalheight=\\textheight-2\\baselineskip,keepaspectratio}\n                \\begin{threeparttable}\n                \\caption{Autoscaling table}\n                    \\input{mytable.tex}\n                \\end{threeparttable}\n            \\end{adjustbox}\n        \\end{table}\n\\end{frame}\n\n\\end{document}\nNote that I use threeparttable for my tables. This isn’t necessary, but it’s fairly standard for me since it handles regression tables nicely. Also note that this expands to the full width and height of the available frame. This is a lot of code for a single figure or table, but can easily be shrunk into a reusable macro.\n\n\n\nScreenshot of autoscaled presentation (including margins for reference)\n\n\n\n\nBonus: autoscaled document\nThe setup is fairly similar for autoscaling figures in a regular document.\n\\documentclass{article}\n\n\\usepackage{graphicx} % Load graphics\n\\usepackage{booktabs} % Nice tables\n\\usepackage{dcolumn} % Booktabs column spacing\n\\usepackage{threeparttable} % Align column caption, table, and notes\n\\usepackage{adjustbox} % Shrink boxes\n\n\\usepackage{showframe} % Useful for debugging\n\n% Flexible notes environment based on minipage\n\\newenvironment{notes}[1][Notes]{\\begin{minipage}[t]{\\linewidth}\\normalsize{\\itshape#1: }}{\\end{minipage}}\n\n\\begin{document}\n\n\\begin{figure}\n    \\caption{Autoscaling image}\n  \\includegraphics[width=\\textwidth,height=\\textheight,keepaspectratio]{myimage.png}\n    \\begin{notes}\n        This is a really fantastic image, even more fantastic for having been autoscaled.\n    \\end{notes}\n\\end{figure}\n\n\\begin{table}\n    \\begin{adjustbox}{width=\\textwidth, totalheight=\\textheight-2\\baselineskip,keepaspectratio}\n        \\begin{threeparttable}\n        \\caption{Autoscaling table}\n            \\input{mytable.tex}\n          \\begin{notes}\n                    * p $&lt;$ 0.1, ** p $&lt;$ 0.05, *** p $&lt;$ 0.01. This is a really fantastic table, even more fantastic for having been autoscaled.\n            \\end{notes}\n        \\end{threeparttable}\n    \\end{adjustbox}\n\\end{table}\n\n\\end{document}\n\n\n\nScreenshot of autoscaled document (including margins for reference)\n\n\nHere I include notes as well, which are more useful for a document (I try to avoid tables with notes, or even better tables at all, in my presentations). In general, however, I prefer to not use adjustbox on my tables in documents to ensure consistent font size throughout (less important in presentations).\n\n\nReference links\n\nScaling graphics automatically\nScaling tables automatically"
  },
  {
    "objectID": "posts/2017-01-19-r-and-julia-again/2017-01-19-r-and-julia-again.html",
    "href": "posts/2017-01-19-r-and-julia-again/2017-01-19-r-and-julia-again.html",
    "title": "Estimating high dimensional fixed effects models in Julia",
    "section": "",
    "text": "Note\n\n\n\nUpdate (December 2022): Julia has changed quite a bit since I originally wrote this. FixedEffectsModels.jl still existed when I last checked in 2018, but it often gave me errors and I used it less and less. As of now, I run regressions (and do the vast, vast majority of my work) using fixest in R.\nI’m trying to use Julia, and specifically FixedEffectsModels.jl, to run fixed effects regressions more quickly. There are complications:\nFirst, I create some fake data in R and export it as both as CSV and as feather.\nImportant: right now, FixedEffectModels.jl uses DataArrays.jl, a deprecated package that deals poorly with factor variables and with NA values. Until it incorporates CategoricalArrays.jl (which handles both nicely), I have to be careful with what I pass from R. In particular, I can’t pass data.frames with NA values or with factor variables. Instead, I have to pass a substitute categorical variable that is actually just an integer. This can be done in R fairly easily, as long as I run relevel first. Then, I can run this:"
  },
  {
    "objectID": "posts/2017-01-19-r-and-julia-again/2017-01-19-r-and-julia-again.html#old-version-of-this-post",
    "href": "posts/2017-01-19-r-and-julia-again/2017-01-19-r-and-julia-again.html#old-version-of-this-post",
    "title": "Estimating high dimensional fixed effects models in Julia",
    "section": "OLD VERSION OF THIS post",
    "text": "OLD VERSION OF THIS post\nI had heard about Julia at various points, usually by people in the data science/Kaggle sphere, but didn’t know much about it. However, for my JMP I need to run a few very big regressions, on the order of a billion observations with multiple dimensions of many-leveled fixed effects. reghdfe(https://github.com/sergiocorreia/reghdfe) and lfe(https://cran.r-project.org/web/packages/lfe/index.html) are the weapons of choice in Stata and R, respectively, for these kinds of models but someone while poking around on the internet I ran across FixedEffectModels.jl. Click that link and compare the speeds. The graph is killer, and it’s not some rigged example - real world experience bears it out.\nSuddenly learning Julia became a lot more interesting. I figured, heck, I know Python and R - how hard could it be? It’s actually pretty hard. Julia is meant to be easy to code in (like Python) but fast as hell (like C). And it IS easy to code in, sort of. But there are some gotchas to learning a relatively recent language:\n\nSparse documentation: the size and depth of both the official Julia documentation, the StackOverflow tag, and the Julia boards are just smaller than Python. In Python, any question you can think of has been asked many, many times, and it’s easy to find. In Julia, your question may have been asked but it’s gonna be hard to find it, and there’s no guarantee it’s been asked at all.\nChanging functionality: Julia has changed a lot, so pulling up old documentation or answered questions is actually a problem. For example, there used to be an sqldf function in the SQLite package that automatically converted the result set into a DataFrame. No longer, so far as I know.\nTyping: Typing is a lot trickier than it is in Python or R. typeof() is super valuable, and the typing conventions make sense, but it’s a lot to learn. Julia is also very picky about Null and NA values.\n“Naturalness”: In both Python in R, once you achieve a certain level of comfort you can type things that you think will work and, generally, they will. This is not also true in Julia. Many things DO work intuitively, but having gotten used to the vectorized nature of R in particular I expect certain actions (changing an entire column, for example) to be easy.\n\nA big part of the issue is that I’m facing is not in getting FixedEffectModels.jl to work, it’s just getting my huge dataset formatted in a way that I can run the darn thing. In principle I could use DataRead.jl, but I haven’t figured out how to compile ReadStat on a PC (the server I’m using). So instead I’m limited to either importing via cleaned CSV or SQLite database. SQLite is much faster but I have to redo all of the cleaning, while CSV is painfully slow and runs out of memory because Julia gets the types wrong (see: “Naturalness”). So now I’m learning how to import the data through SQLite in chunks, type it properly, and merge in the datasets I use in my Stata cleaning process. All before I get to run a single regression. And three weeks before my JMP needs to be done.\nBut Julia is a nice tool to have, and it seems to have some momentum. The community appears to be tremendous.\nMore notes (I feel like I’m learning a ton about language design):\n\nArrays that accept NAs are nice, but a devil’s bargain. Typed arrays are both faster and consume MUCH less memory, since Any arrays need to contain a pointer for each unit, plus the data itself must be stored somewhere.\nIf your date is in a fixed format, a custom (read: dumb) parser is way better than the smart parser provided in Julia.\nPay close attention your types, if you care about memory allocation. Strings are huge. Floats are remarkably small (and so cool!). Int choices can be important.\nGod help you if you have an NA hiding in your arrays. The @data macro is pretty much worthless. The folks working on Julia is still figuring out how to deal with NA values. DataArrays, which CAN (in general) handle NAs even when typed, have trouble important NA values. I ended up getting around a problem by temporarily converting all NAs in an Array{Any} variable to a real number so that I could convert it to a DataArray{Float16} before re-adding the NAs. The Nullable package is supposed to be a fix for this."
  },
  {
    "objectID": "posts/2021-05-30-making-tables/2021-05-30-making-tables.html",
    "href": "posts/2021-05-30-making-tables/2021-05-30-making-tables.html",
    "title": "Making better tables",
    "section": "",
    "text": "Note\n\n\n\nI now have an updated version of this post. You can find it here.\nThere’s not much that’s sexy about a table. Everyone loves a good figure, but you won’t find many people singing the praises of a particularly well-constructed table in an academic paper. And yet, tables are the most common medium through which academic authors summarize datasets and relay results.\nSo why don’t tables get any love? Maybe it’s because they usually aren’t that interesting. Jammed full of dry, difficult-to-process numbers conveying a range of different types of information (means, standard deviations, parameter estimates, goodness-of-fit numbers, and so on), it’s no wonder that the average reader, myself included, is more inclined to skip over the tables to get to the more fun and illustrative visual displays in the paper.\nBut I don’t think it has to be this way. Or, at least, I think we can do better. The goal of this post is to write up my current best practices for generating the two main types of tables I use: data summary tables and regression tables. I’ll include the R code to generate the tables and the TeX code to include them in the paper, since although I try to avoid writing in TeX as much as possible, it’s still the best option for creating elegant, consistently formatted manuscripts."
  },
  {
    "objectID": "posts/2021-05-30-making-tables/2021-05-30-making-tables.html#general-approach",
    "href": "posts/2021-05-30-making-tables/2021-05-30-making-tables.html#general-approach",
    "title": "Making better tables",
    "section": "General approach",
    "text": "General approach\nSeveral guidelines steer my general approach.\n\nAutomate the table generation process. The idea here is that I should be able to regenerate the table without any “manual” input to the greatest extent possible. This means that the could shoul d everything relating to table formatting.\nUse booktabs because it’s better. As with any form of data presentation, the best-designed tables are as simple as possible. As argued here by Nick Higham, vertical lines and double horizontal lines have no place in tables. The focus should be on the information, not the stuff around it.\nCaptions and notes are part of the text. I find that I prefer to keep captions and notes separate from the table generation process. Writing and improving these pieces of text are closer to the writing process for me, and going back to the table generation code to edit captions or notes is a chore."
  },
  {
    "objectID": "posts/2021-05-30-making-tables/2021-05-30-making-tables.html#generating-data-summary-tables-in-r",
    "href": "posts/2021-05-30-making-tables/2021-05-30-making-tables.html#generating-data-summary-tables-in-r",
    "title": "Making better tables",
    "section": "Generating data summary tables in R",
    "text": "Generating data summary tables in R\nAlso known as summary statistics, or descriptive statistics, data summary tables deliver summarizing information about the variables used in analyses.\nWe’ll use the Fatalities dataset from the AER package as our sample dataset. This is a panel dataset with around 30 variables relating to traffic fatalities in the United States. First, we’ll do some quick setup, load the data, and preview it using glimpse (a much more useful alternative to head, in my view).\n\npacman::p_load(tidyverse, kableExtra, AER)\n\n# Load fatalities panel data\ndata(\"Fatalities\")\nglimpse(Fatalities)\n\nRows: 336\nColumns: 34\n$ state        &lt;fct&gt; al, al, al, al, al, al, al, az, az, az, az, az, az, az, a…\n$ year         &lt;fct&gt; 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1982, 1983, 198…\n$ spirits      &lt;dbl&gt; 1.37, 1.36, 1.32, 1.28, 1.23, 1.18, 1.17, 1.97, 1.90, 2.1…\n$ unemp        &lt;dbl&gt; 14.4, 13.7, 11.1, 8.9, 9.8, 7.8, 7.2, 9.9, 9.1, 5.0, 6.5,…\n$ income       &lt;dbl&gt; 10544.15, 10732.80, 11108.79, 11332.63, 11661.51, 11944.0…\n$ emppop       &lt;dbl&gt; 50.69204, 52.14703, 54.16809, 55.27114, 56.51450, 57.5098…\n$ beertax      &lt;dbl&gt; 1.53937948, 1.78899074, 1.71428561, 1.65254235, 1.6099070…\n$ baptist      &lt;dbl&gt; 30.3557, 30.3336, 30.3115, 30.2895, 30.2674, 30.2453, 30.…\n$ mormon       &lt;dbl&gt; 0.32829, 0.34341, 0.35924, 0.37579, 0.39311, 0.41123, 0.4…\n$ drinkage     &lt;dbl&gt; 19.00, 19.00, 19.00, 19.67, 21.00, 21.00, 21.00, 19.00, 1…\n$ dry          &lt;dbl&gt; 25.0063, 22.9942, 24.0426, 23.6339, 23.4647, 23.7924, 23.…\n$ youngdrivers &lt;dbl&gt; 0.211572, 0.210768, 0.211484, 0.211140, 0.213400, 0.21552…\n$ miles        &lt;dbl&gt; 7233.887, 7836.348, 8262.990, 8726.917, 8952.854, 9166.30…\n$ breath       &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no, n…\n$ jail         &lt;fct&gt; no, no, no, no, no, no, no, yes, yes, yes, yes, yes, yes,…\n$ service      &lt;fct&gt; no, no, no, no, no, no, no, yes, yes, yes, yes, yes, yes,…\n$ fatal        &lt;int&gt; 839, 930, 932, 882, 1081, 1110, 1023, 724, 675, 869, 893,…\n$ nfatal       &lt;int&gt; 146, 154, 165, 146, 172, 181, 139, 131, 112, 149, 150, 17…\n$ sfatal       &lt;int&gt; 99, 98, 94, 98, 119, 114, 89, 76, 60, 81, 75, 85, 87, 67,…\n$ fatal1517    &lt;int&gt; 53, 71, 49, 66, 82, 94, 66, 40, 40, 51, 48, 72, 50, 54, 3…\n$ nfatal1517   &lt;int&gt; 9, 8, 7, 9, 10, 11, 8, 7, 7, 8, 11, 19, 16, 14, 5, 2, 2, …\n$ fatal1820    &lt;int&gt; 99, 108, 103, 100, 120, 127, 105, 81, 83, 118, 100, 104, …\n$ nfatal1820   &lt;int&gt; 34, 26, 25, 23, 23, 31, 24, 16, 19, 34, 26, 30, 25, 14, 2…\n$ fatal2124    &lt;int&gt; 120, 124, 118, 114, 119, 138, 123, 96, 80, 123, 121, 130,…\n$ nfatal2124   &lt;int&gt; 32, 35, 34, 45, 29, 30, 25, 36, 17, 33, 30, 25, 34, 31, 1…\n$ afatal       &lt;dbl&gt; 309.438, 341.834, 304.872, 276.742, 360.716, 368.421, 298…\n$ pop          &lt;dbl&gt; 3942002, 3960008, 3988992, 4021008, 4049994, 4082999, 410…\n$ pop1517      &lt;dbl&gt; 208999.6, 202000.1, 197000.0, 194999.7, 203999.9, 204999.…\n$ pop1820      &lt;dbl&gt; 221553.4, 219125.5, 216724.1, 214349.0, 212000.0, 208998.…\n$ pop2124      &lt;dbl&gt; 290000.1, 290000.2, 288000.2, 284000.3, 263000.3, 258999.…\n$ milestot     &lt;dbl&gt; 28516, 31032, 32961, 35091, 36259, 37426, 39684, 19729, 1…\n$ unempus      &lt;dbl&gt; 9.7, 9.6, 7.5, 7.2, 7.0, 6.2, 5.5, 9.7, 9.6, 7.5, 7.2, 7.…\n$ emppopus     &lt;dbl&gt; 57.8, 57.9, 59.5, 60.1, 60.7, 61.5, 62.3, 57.8, 57.9, 59.…\n$ gsp          &lt;dbl&gt; -0.022124760, 0.046558253, 0.062797837, 0.027489973, 0.03…\n\n\nThat’s a lot of variables! But we’re not going to summarize all of them. Suppose we think the main drivers of traffic fatalities in a state are the price of beer, how much people make, how much people drive, and how many people live there. If those are the variables we’re focused on, we might want to give the reader a sense for their distributions to help contextualize the remainder of the paper. That’s really the goal of a good summary statistics table: to provide context.\nThere are some great tools that will generate summaries for you (my favorite is modelsummary::datasummary), but I find that most of the time I prefer to build them by hand. function.] This gives me a clearer sense for what is in the data, and ensures that I don’t get lazy and just generate some default set of statistics, since those are not terribly informative or easy to read.\n\ndata &lt;- Fatalities %&gt;% ## Rescale variables\n    mutate(beertax = beertax * 100,\n           income = income / 1000,\n           miles = miles / 1000,\n           fatal = fatal / 1000,\n           pop = pop / 100000) \n\nsummary &lt;- data %&gt;%\n    select(State = state, \n           Year = year, \n           `Unemployment rate` = unemp, \n           Income = income, \n           `Beer tax (cents)` = beertax, \n           `Miles per driver (1000s)` = miles, \n           `Vehicle fatalities (1000s)` = fatal, \n           `Population (m)` = pop) %&gt;%\n    pivot_longer(-c(State, Year)) %&gt;%\n    group_by(` ` = name) %&gt;% \n    summarize(Mean = mean(value), \n              Median = median(value), \n              P5 = quantile(value, p = 0.05), \n              P95 = quantile(value, p = 0.95))\nsummary\n\n# A tibble: 6 × 5\n  ` `                          Mean Median     P5    P95\n  &lt;chr&gt;                       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Beer tax (cents)           51.3   35.3    9.47  162.  \n2 Income                     13.9   13.8   10.8    18.1 \n3 Miles per driver (1000s)    7.89   7.80   6.08    9.69\n4 Population (m)             49.3   33.1    6.20  164.  \n5 Unemployment rate           7.35   7      3.80   11.8 \n6 Vehicle fatalities (1000s)  0.929  0.701  0.116   2.83\n\n\nIf you’re familiar with the tidyverse this code will be fairly intuitive. I rescale the variables so that they display well with the same number of digits1, select the variables I want to display, pivot the data into a long format, and finally summarize all of the variables using the same summary functions. I like re-labeling both the variables and the summary functions here to their final, human-readable versions that I’ll use in the table, since it reduces redundancy and extra typing.\nThe final step is to actually save the TeX output. kableExtra::kbl does basically all of the work for us here.\n\nkbl(summary, \n    format = \"latex\", \n    linesep = \"\",\n    digits = 1, \n    booktabs = T) %&gt;%\n    print()\n\n\n\\begin{tabular}[t]{lrrrr}\n\\toprule\n  & Mean & Median & P5 & P95\\\\\n\\midrule\nBeer tax (cents) & 51.3 & 35.3 & 9.5 & 162.1\\\\\nIncome & 13.9 & 13.8 & 10.8 & 18.1\\\\\nMiles per driver (1000s) & 7.9 & 7.8 & 6.1 & 9.7\\\\\nPopulation (m) & 49.3 & 33.1 & 6.2 & 164.5\\\\\nUnemployment rate & 7.3 & 7.0 & 3.8 & 11.8\\\\\nVehicle fatalities (1000s) & 0.9 & 0.7 & 0.1 & 2.8\\\\\n\\bottomrule\n\\end{tabular}\n\n\nI use print() to display the output here, but normally I would write this to a file using %&gt;% write(\"summary.tex\"), which would give us this TeX code in a file that we can load using our main document. But before we do that, we have one more task…"
  },
  {
    "objectID": "posts/2021-05-30-making-tables/2021-05-30-making-tables.html#generating-regression-tables-in-r",
    "href": "posts/2021-05-30-making-tables/2021-05-30-making-tables.html#generating-regression-tables-in-r",
    "title": "Making better tables",
    "section": "Generating regression tables in R",
    "text": "Generating regression tables in R\nThat’s right, it’s time to regress!! Linear regression is most economists’ analytic weapon of choice, including mine. I like the fixest package for many reasons, but for our purposes today the most salient one is that it includes a very nice table-making function called etable (again, for a very good alternative see modelsummary, especially if you want to make HTML tables).\nFirst, let’s estimate a few models.\n\nlibrary(fixest)\nmodels &lt;- list()\nmodels[[\"OLS\"]] &lt;- feols(fatal ~ unemp + income + beertax + miles, data = data)\nmodels[[\"+ State FE\"]] &lt;- feols(fatal ~ unemp + income + beertax + miles | state, data = data)\nmodels[[\"+ Year FE\"]] &lt;- feols(fatal ~ unemp + income + beertax + miles | state + year, data = data)\n\nHere we’ve estimated three models that vary only in their included fixed effects. The first model, “OLS”, includes only the covariates. The second and third add state and year fixed effects, respectively. This kind of buildup table shows how the parameter estimates change as we condition on more covariates (which are just the additional fixed effects here). For the record, this is just a demonstration exercise; I wouldn’t interpret any of these coefficients as very likely to represent their true “causal” analogues. To show the results, I’ll use the etable function.\n\ndict_names &lt;- c(\"fatal\" = \"Vehicle fatalities (1000s)\",\n                \"unemp\" = \"Unemployment rate\",\n                \"income\" = \"Income\",\n                \"beertax\" = \"Beer tax (cents)\",\n                \"miles\" = \"Miles per driver (1000s)\",\n                \"pop\" = \"Population (m)\",\n                \"state\" = \"State\",\n                \"year\" = \"Year\")\n\netable(models,\n       cluster = \"state\",\n       dict = dict_names,\n       drop = \"Intercept\",\n       digits = \"r2\",\n       digits.stats = 2,\n       fitstat = c(\"n\", \"war2\"),\n       style.tex = style.tex(\"aer\",\n                             fixef.suffix = \" FEs\",\n                             fixef.where = \"var\",\n                             yesNo = c(\"Yes\", \"No\")),\n       tex = T) %&gt;%\n    print()\n\n\\begingroup\n\\centering\n\\begin{tabular}{lccc}\n   \\toprule\n    & \\multicolumn{3}{c}{Vehicle fatalities (1000s)}\\\\\n                            & (1)          & (2)           & (3)\\\\  \n   \\midrule \n   Constant                 & -3.40$^{**}$ &               &   \\\\   \n                            & (1.65)       &               &   \\\\   \n   Unemployment rate        & 0.14$^{***}$ & -0.02$^{***}$ & -0.03$^{***}$\\\\   \n                            & (0.05)       & (0.01)        & (0.01)\\\\   \n   Income                   & 0.22$^{**}$  & 0.02          & 0.04$^{*}$\\\\   \n                            & (0.09)       & (0.01)        & (0.02)\\\\   \n   Beer tax (cents)         & 0.00$^{***}$ & 0.00$^{**}$   & 0.00$^{*}$\\\\   \n                            & (0.00)       & (0.00)        & (0.00)\\\\   \n   Miles per driver (1000s) & -0.01        & 0.00          & 0.00\\\\   \n                            & (0.05)       & (0.00)        & (0.00)\\\\   \n    \\\\\n   State FEs                & No           & Yes           & Yes\\\\  \n   Year FEs                 & No           & No            & Yes\\\\  \n    \\\\\n   Observations             & 336          & 336           & 336\\\\  \n   Within Adjusted R$^2$    &              & 0.25          & 0.24\\\\  \n   \\bottomrule\n\\end{tabular}\n\\par\\endgroup\n\n\nNote that we used the dict_names character vector to define character labels. This unfortunately does repeat some code from earlier (and could have been avoided if I had been a bit more clever), but it’s such an elegant want to handle labeling that I wanted to highlight how it’s used in etable. Note also that rather than using a booktabs argument, the style.tex argument is doing the heavy lifting on the design side. You can review the etable documentation for more, but basically I’m asking it to follow the general American Economic Review (AER) format, which happens to include booktabs-liek tables, with a few additional modifications.\nSo, we have two tabular .tex files (assuming we write this one as well). Now what?"
  },
  {
    "objectID": "posts/2021-05-30-making-tables/2021-05-30-making-tables.html#the-final-product",
    "href": "posts/2021-05-30-making-tables/2021-05-30-making-tables.html#the-final-product",
    "title": "Making better tables",
    "section": "The final product",
    "text": "The final product\nIt’s time to put it all together! Below is a minimal “container” TeX code for these two tables, with sample captions and notes.\n\\documentclass{article}\n\\usepackage{threeparttable}\n\\usepackage{booktabs}\n\\usepackage[capitalise]{cleveref}\n\n% Define a notes environment\n\\newenvironment{notes}[1][Notes]{\\begin{minipage}[t]{\\linewidth}\\small{\\itshape#1: }}{\\end{minipage}}\n\n\\begin{document}\n\n\\cref{tab:summary} documents summary statistics. \\cref{tab:regs} shows regression results.\n\n\\begin{table}[!h]\n    \\centering\n    \\begin{threeparttable}\n        \\caption{Data summary}\n        \\label{tab:summary}\n        \\input{summary.tex}\n        \\begin{notes}\n        This table summarizes the variables used in the study.\n        \\end{notes}\n    \\end{threeparttable}\n\\end{table}\n\n\\begin{table}[!h]\n    \\centering\n    \\begin{threeparttable}\n        \\caption{Regression results}\n        \\label{tab:regs}\n        \\input{regs.tex}\n        \\begin{notes}\n        This table documents regression results.\n        \\end{notes}\n    \\end{threeparttable}\n\\end{table}\n\n\\end{document}\nAnd here’s a screenshot of what that looks like once compiled.\n\nSo… maybe there IS something a little sexy about tables?"
  },
  {
    "objectID": "posts/2020-07-03-plot-raster-shape/2020-07-03-plot-raster-shape.html",
    "href": "posts/2020-07-03-plot-raster-shape/2020-07-03-plot-raster-shape.html",
    "title": "Overlaying a raster and shapefile",
    "section": "",
    "text": "I’m often overlaying rasters with shapefiles in order to get, for example, the average weather for Indonesia. I’ve found that it’s immensely important that I map my data when I’m doing this sort of thing, to make sure that I’m not making any boneheaded mistakes (e.g., using the wrong projection). Here’s an example of a map like that, where the color of the cells indicates whether or not we have data there, plus the code I used to create it.\n\n\n\nIndonesia + UDel precip raster overlay\n\n\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(raster)\nlibrary(cowplot)\n\n# Load UDEL raster data\n# Source: ftp://ftp.cdc.noaa.gov/Datasets/udel.airt.precip/precip.mon.ltm.v501.nc\nrast &lt;- raster(\"precip.mon.ltm.v501.nc\")\n\n# Load Indonesia shapefile, tranform to raster CRS, and simplify for performance\n# Source: https://gadm.org/download_country_v3.html\npoly &lt;- readRDS(\"IDN_adm1.sf.rds\") %&gt;% \n  st_transform(proj4string(rast)) %&gt;%\n  st_simplify(0.01, preserveTopology = TRUE)\n\n# Crop global raster to extent of polygon\nrast &lt;- crop(rast, extent(poly))\n\nrast_df &lt;- as.data.frame(rast, xy = TRUE)\nnames(rast_df)[3] &lt;- \"value\"\n\n# Make a plot with the shapefile and boxes for the raster cells, where gray cells indicate no data.\np &lt;- ggplot(data = rast_df) + \n  geom_sf(data = poly, fill = NA, colour = \"blue\", size = 0.25) + \n  geom_tile(data = rast_df %&gt;% filter(is.na(value)), mapping = aes(x = x, y = y), size = 0.25, fill = NA, color = alpha(\"gray\", 0.25)) +\n  geom_tile(data = rast_df %&gt;% filter(!is.na(value)), mapping = aes(x = x, y = y), size = 0.25, fill = NA, color = alpha(\"red\", 0.5)) +\n  theme_map()\n\nsave_plot(\"map.png\", plot = p)"
  },
  {
    "objectID": "posts/2018-04-12-svycontrasts/2018-04-12-svycontrasts.html",
    "href": "posts/2018-04-12-svycontrasts/2018-04-12-svycontrasts.html",
    "title": "Linear combinations of coefficients in R",
    "section": "",
    "text": "Note\n\n\n\nI still use a version of this occasionally, but the various marginal effects package – specifically `marignaleffects’ seem to replicate most of the functionality I need these days.\n\n\nOne of the few features I miss from Stata is the very-intuitive lincom command. Most of the time I recreate that functionality with survey::svycontrast. But, I always forget the syntax. This code demonstrates a minimal working example.\n\nlibrary(survey)\n\nN &lt;- 100\ndf &lt;- data.frame(x1 = rnorm(N), x2 = rnorm(N))\ndf$y &lt;- 1 + 3 * df$x1 - 2 * df$x2 + rnorm(N, 0, 0.5)\n\nfit &lt;- lm(y ~ x1 + x2, data = df)\n\nHaving fit the model, we can pass unnamed vector with the right number of coefficients to get our desired linear combination:\n\nsvycontrast(fit, c(0, 1, 1))\n\n         contrast     SE\ncontrast  0.88835 0.0639\n\n\nOr a named vector with any number of coefficients (as long as the names match). Or a list of named vectors. One “gotcha” to keep in mind: I have found that the latter syntax fails for some versions of survey and may be OS-dependent.\n\nsvycontrast(fit, list(c(\"x2\" = 1, \"x1\" = 1), c(\"x2\" = 4, \"x1\" = 2)))\n\n     contrast     SE\n[1,]  0.88835 0.0639\n[2,] -2.32800 0.2075\n\n\nIf you want an easy way to pass svycontrast a data.frame of where each row is a different linear combination (and each column a different variable), below is how you do that. This is usually if you want to produce similar behavior to, say, running predict on a fitted model but A) the predict call for that model doesn’t return standard errors (as with felm or fixest) and/or B) you only want to linearly combine some of the variables (equivalent to setting all other coefficients = 0). In either case, this will work:\n\nsvycontrast_df &lt;- function(fit, newdata) {\n  # Call surveyconstrast with a data frame \n  df &lt;- newdata\n  \n  # Transform data.frame into a list of its row vectors\n  df_list &lt;- as.list(as.data.frame(t(newdata))) \n  df_list &lt;- lapply(df_list, setNames, colnames(df)) # Set all character vector names inside list\n  \n  # Return a named list\n  setNames(as.list(as.data.frame(svycontrast(fit, df_list))), c(\"est\", \"se\"))\n}\n\nnewdata &lt;- data.frame(x1 = seq(-3, 3, 1))\nnewdata$x2 &lt;- newdata$x^2\nsvycontrast_df(fit, newdata)\n\n$est\n[1] -27.2932457 -14.0907987  -4.9930501   0.0000000   0.8883517  -2.3279951\n[7]  -9.6490403\n\n$se\n[1] 0.44004435 0.20524188 0.06296086 0.00000000 0.06389291 0.20753339 0.44365681"
  },
  {
    "objectID": "posts/2022-02-09-productivity/2022-02-09-productivity.html",
    "href": "posts/2022-02-09-productivity/2022-02-09-productivity.html",
    "title": "Productivity and the work habits that work (for me)",
    "section": "",
    "text": "Note\n\n\n\n(Original post April 10, 2021, updated February 9, 2022. This blog post is discussed on the excellent Hidden Curriculum podcast with Alex Hollingsworth and Sebastian Tello-Trillo.)\nI used to think a lot about productivity. I still do, but I used to, too. (Joke credit to the late, great Mitch Hedberg.)\nWhat is productivity? The boring economics-y definition is “rate of output per unit of input,” which is accurate but mostly unhelpful. A more human definition, and one I prefer, is “doing good work in a sustainable way.” So a productive week for me is one where I generate valuable pieces of work, like cleaned data, lesson plans, or sections of a paper (“good work”) while also staying happy and healthy by doing other non-work things that I love (“in a sustainable way”). Not all weeks feel that way, but over the years I’ve found that some behaviors are more likely to lead to productive weeks than others. And turning these good behaviors into habits has helped me get more done with less time and frustration than I did early in my career.\nDeveloping those habits is what this post is about. In a previous and now slightly cringe-inducing note, I wrote about the kinds of practices and tools that I found useful for maintaining productivity as a grad student. In the five years since I wrote that, my thinking has continued to evolve. The following is a loose meander through that thinking.\nI’ll start with some necessary caveats, then discuss the principles that underpin my thoughts about productivity, list a few of my favorite habits (including some that are more specific to my job as a researcher), and link to some of the tools that I currently use. I’ll wrap up with a brief list of some resources that have helped me form these ideas."
  },
  {
    "objectID": "posts/2022-02-09-productivity/2022-02-09-productivity.html#caveats",
    "href": "posts/2022-02-09-productivity/2022-02-09-productivity.html#caveats",
    "title": "Productivity and the work habits that work (for me)",
    "section": "Caveats",
    "text": "Caveats\n\nThe ideas in this post are aspirational. I can assure you that I fail to demonstrate these habits all the time. In fact, I would guess that I fail more often than I succeed. I suspect that most people who write about productivity are the same way, though you might not know it from their marketing copy. We’re all human here, and it’s worth remembering that incremental improvement is still improvement.\nWhat works for me may not work for you. We’re all different, with different abilities and challenges. If something you read here doesn’t make sense or doesn’t work for you, ignore it! Just thinking carefully what kind of habits are likely to lead to a healthy and productive life for you is the most important part of the process.\n\nWith those out of the way, let’s get started!"
  },
  {
    "objectID": "posts/2022-02-09-productivity/2022-02-09-productivity.html#principles",
    "href": "posts/2022-02-09-productivity/2022-02-09-productivity.html#principles",
    "title": "Productivity and the work habits that work (for me)",
    "section": "Principles",
    "text": "Principles\nI have a few of principles I try keep in mind when I think about being productive.\n\nThe goal is to get stuff done. This seems obvious, but I’ve found it’s often easy to get diverted onto low-value tasks that offer quick dopamine hits, rather than to spend time on the stuff that really matters. We all have metrics that we use to determine if we’ve been successful. Some might be given to us externally, some we choose on our own. If you’re a researcher like me, your “stuff” is (mainly) writing good research papers and teaching well. It is not, for example, writing as many emails as possible or identifying the best possible note-taking tool. (I’m guilty of spending too much time on both of those, by the way.) A good litmus test here is to ask yourself: will I be happy a year from now that I accomplished this task?\nWriting IS thinking. My sense is that most people think writing is simply transcribing ideas from your brain. It is not. Writing is a critical part of the creative process and is far more generative (in terms of leading to new ideas) than we realize. When you force yourself to write down something you will refine it, improve on it, and probably come up with something new entirely. Waiting to write until they “have a good idea” is one of the most common mistakes I see in early-career PhD students. (Note to self: a full post on how to structure writing so it can lead to good ideas is probably in order at some point.)\nDon’t use your brain as a hard drive. Free your brain to do what it is best at: sparking ideas! Don’t use it to retain TODO lists, guidance for how to best code something, new projects that you want to start, your schedule for the week, long-term goals, or frankly anything that could easily be stored on a sheet of paper or computer. Make a note somewhere retrievable and let your brain move on to something else!\nFuture-proof what you can. Consider the computer and the programs you’re using right now. Were you using that same computer and those same programs ten years ago? Even five years ago? I doubt it. I also doubt you’ll be using the same computer and programs five or ten years in the future. So future-proof whatever you can. That means: use systems that are easily backed up and recognize that you will probably change tools soon (if you’re like me, very likely within six months), so don’t overinvest in the ones you’re using now, and make sure that you’ll be able to get the data out of them easily."
  },
  {
    "objectID": "posts/2022-02-09-productivity/2022-02-09-productivity.html#habits",
    "href": "posts/2022-02-09-productivity/2022-02-09-productivity.html#habits",
    "title": "Productivity and the work habits that work (for me)",
    "section": "Habits",
    "text": "Habits\nWith those principles in mind, the following are a semi-categorized list of habits that I aspire to. Many of these are either stolen directly from or indirectly inspired by one or more of the resources I list below, so I claim no credit for them. In particular, Make Time, by Jake Knapp and John Zeratsky, is probably my favorite book on the topic and the source for many of these ideas.\n\nProtect your productive times of day. Mornings are my most creative and productive times, so I schedule almost all of my meetings in the afternoons. (Some people find that they are most productive and creative at night. Adjust accordingly.)\nWrite first. My absolute best days are the ones where I start the day with thirty minutes of writing.\nMomentum matters. Start a larger set of tasks by taking on a small, achievable goal. Get the momentum going!\nCommit to getting your written work in front of someone else as soon as possible. As I said above, writing is thinking. Writing for an audience is better thinking. Writing for an audience that you expect to edit your work is the best thinking.\nDeadlines are highly motivating. Submit to conferences, agree to presentations, commit to returning drafts by a certain date. Don’t wait until you feel the work is “ready” (it never will be), give yourself a deadline.\nYour brain hates a bad plan. Writing down a plan for the day/week/month is almost always time well spent.\nPick a “highlight” to be the 60 to 90 minute centerpiece of your day. It’s not the only thing you’ll do that day, but think of it as the thing that you’ll reflect on later as your major accomplishment for the day. It doesn’t have to be work, though!\nBlock your schedule. By blocking off writing time, or even writing time for a particular project, you can free your mind from stressing about that project at other times.\nEvery task should have a deliverable. A deliverable is a specific product of your work. Having tasks linked to deliverables keeps you accountable to yourself (and others, if you’re collaborating). A bad task would be something like “work on the data section,” which isn’t specific and doesn’t involve a deliverable. A better task would be “write the paragraph of the paper that describes how the weather data were collected.”\nPreempt distractions. If you have horrible impulse control, like me, minimize distractions by knowing what triggers you to waste time. For example, I will often leave my phone either in my bag or in my car.\nHave a plan for what you’re about to do before you open a screen, including your laptop.\nIn general, I know that I can lose large chunks of time to infinite scroll apps (Instagram, Twitter, Facebook, …), so I keep them off my phone and I don’t stay logged in on my computer. Adding a small effort cost for accessing these apps seems to help me avoid them.\nThe nights when I work late into the night almost always result in a loss of energy and motivation the next day. Instead, shut it down while you still have every and something to dive into tomorrow. Go read a book, or hang out with friends or family. It’s better to maintain a constant stream of work and to enjoy your leisure time. Leisure time is recharge time.\nNot everyone is like this, but I absolutely require exercise. I’ve come to know when I’m likely to hit lulls and try to aim my workouts for those times. That’s usually just before lunch or around 2 or 3:30.\n“No” is the most important word in the world. Especially as a junior academic, you will be tempted to say yes to projects, tasks, committees, and so on. Nobody knows what is on your plate except for you, and it’s your job to protect your time. The people who want you to succeed will understand when you decline. (I say this with some hesitancy, since I’m aware that folks who look like me - white men - do not receive as many asks as our minority or female colleagues. So let me caveat this advice with the suggestion that if you do look like me, please do consider who is likely to take on this task if you aren’t. These are not easy dilemmas.)\nEmail is evil. It really is. Anything you can do to limit the flood of mail is enormously important. Very few people reading this will have their job performance determined by how many emails they responded to.\nDon’t neglect your mental health. Journaling, meditating, therapy, whatever works for you. I do none of them all the time, but all of them some of the time.\nDon’t obsess about productivity. It’s possible to overdo it and to become focused on checking off as many tasks as possible. I’ve been there, and it can become joyless. Having a highlight helps here.\n\n\nResearch-specific habits\nIf you’re a researcher like me, there are a few habits that are particularly useful for our field (but likely for others, too).\n\nBe ruthless about your project load. I have found that I can, at most, actively push forward two research projects at once. If I try to spread my scarce research time over four or five projects, nothing actually gets done. Switching costs are real and large, so it’s better to focus intensely one one thing for, say, a six week sprint, than to spread yourself thin across five or six projects all the time. (Note: if you manage a research team, the math here is a bit different, but the general principle of focusing your own active research time holds.)\nGet to a minimum viable research product. As they say in silicon valley, “fail fast.” Ask yourself what would kill your project, and try to figure out if it’s likely to do so as quickly as possible.\nMock up data to write clean code. Whenever I write code on invented dataset, it always works out better for me. Rather than deal the the messy, real dataset I can make progress on only the critical pieces. And I know they’re critical because I had to construct the data. Then when I apply it to the real data, I’m much less likely to get caught up in the details.\nPre-comment your code. Instead of just starting with code, start by describing what you want the code to do. First line of a piece of code should describe what that code will do in general. Then, you can generate a series of comments that outline the functions you’ll need, the steps you’ll take, and so on.\nAgain, write for an audience. The main reason why I generate these blog posts is actually self-serving. By writing for an audience, I force myself to be clear and concise (at least as much as I am capable). In the end, the posts are helpful for me even years down the line."
  },
  {
    "objectID": "posts/2022-02-09-productivity/2022-02-09-productivity.html#tools",
    "href": "posts/2022-02-09-productivity/2022-02-09-productivity.html#tools",
    "title": "Productivity and the work habits that work (for me)",
    "section": "Tools",
    "text": "Tools\nI hesitate to discuss specific tools too much (see Principle #4 above), since I know how mercurial I am with what I use. But for the record, here’s the set of applications that are most often open on my computer.\n\nMarkdown for as much as possible.\n\nWriting, preparing lecture slides, note-taking. I even make this website using Markdown (well, blogdown).\nMy weapon of choice is Atom. Other people like Sublime Text.\n\nR/RStudio for almost all of my coding / analysis tasks. I used to use more Python and Stata, but R does almost everything I neeed now.\nAlfred (OS X) to avoid hunting through my file structure. It’ll change your life.\nNotion: Note-taking, personal task management. There are lots of tools like Notion: I previously used Roam Research, and before that Bear, but they’re all mostly interchangeable. I just need something that allows me to quickly get ideas out of my brain and somewhere where I can find them later.\nGithub for project management, especially across a team. (I could write a whole post on how to use Github Issues for research.)"
  },
  {
    "objectID": "posts/2022-02-09-productivity/2022-02-09-productivity.html#resources",
    "href": "posts/2022-02-09-productivity/2022-02-09-productivity.html#resources",
    "title": "Productivity and the work habits that work (for me)",
    "section": "Resources",
    "text": "Resources\n\nMake Time (Jake Knapp and John Zeratsky). This is probably my favorite single resource on productivity. I find it much more reasonable and accessible than many of the other resources out there, which can be dogmatic.\nMost of Cal Newport’s books are worth a read.\nGetting Things Done (David Allen): A system for planning work that’s been useful in shaping my thinking.\nPARA System (Tiago Forte): Helped guide how I set up my document structure. It’s not perfect, but I haven’t found anything better yet. On the other hand, Alfred (see above) has basically obviated having a good document structure for me."
  },
  {
    "objectID": "posts/2022-02-09-productivity/2022-02-09-productivity.html#wrapping-up",
    "href": "posts/2022-02-09-productivity/2022-02-09-productivity.html#wrapping-up",
    "title": "Productivity and the work habits that work (for me)",
    "section": "Wrapping up",
    "text": "Wrapping up\nI hope you’ve found something useful here. I’ve noticed that documenting these ideas and discussing them with friends and colleagues has often led me to insightful conversations about how all of us can improve our ability to do good work (sustainably). I’m hopeful that this post will spark some useful ideas for you as well, and if you have any thoughts I’d love to hear them!"
  },
  {
    "objectID": "posts/2016-05-18-prism-leads/2016-05-18-prism-leads.html",
    "href": "posts/2016-05-18-prism-leads/2016-05-18-prism-leads.html",
    "title": "Yesterday’s Maximum Temperature is… Today’s Maximum Temperature?",
    "section": "",
    "text": "Note\n\n\n\nThis was also posted on G-Feed.\n\n\nYou may not know this, but Kahlil Gibran was actually thinking about weather data when he wrote that yesterday is but today’s memory, and tomorrow is today’s dream. (Okay, not really.)\nBad literary references aside, readers of this blog know that climate economists project the impacts of climate change is by observing the relationships between historical weather realizations and economic outcomes. Fellow former ARE PhD Kendon Bell alerted me to an idiosyncrasy in one of the weather datasets we frequently use in our analyses. Since many of us (myself included) rely on high-quality daily weather data to do our work, I investigated. This post is a fairly deep dive into what I learned, so if you happen to not be interested in the minutiae of daily weather data, consider yourself warned.\nThe PRISM AN81-d dataset is daily minimum and maximum temperatures, precipitation, and minimum and maximum vapor pressure deficit data for the continental United States from 1981 to present. It is created by the PRISM Climate Group at Oregon State, and it is really nice. Why? It’s a gridded data product: it is composed of hundreds of thousands of 4km by 4km grid cells, where the values for each cell are determined by a complex interpolation method from weather station data (GHCN-D) that accounts for topological factors. Importantly, it’s consistent: there are no discontinuous jumps in the data (see figure below) and it’s a balanced panel: the observations are never missing.\nSource: PRISM Climate Group\nThese benefits are well-understood, and as a result many researchers use the PRISM dataset for their statistical models. However, there is a particularity of these data that may be important to researchers making use of the daily variation in the data: most measurements of temperature maximums, and some measurements of temperature minimums, actually refer to the maximum or minimum temperature of the day before the date listed.\nTo understand this, you have to understand that daily climate observations are actually summaries of many within-day observations. The reported maximum and minimum temperature are just the maximum and minimum temperature observations within a given period, like a day. The tricky part is that stations define a “day” as “the 24 hours since I previously recorded the daily summary”, but not all stations record their summaries at the same time. While most U.S. stations record in the morning (i.e, “morning observers”), a hefty proportion of stations are either afternoon or evening observers. PRISM aggregates data from these daily summaries, but in order to ensure consistency tries to only incorporate morning observers. This leads to the definition of a “PRISM day”. The PRISM documentation defines a “PRISM day” as:\n\nStation data used in AN81d are screened for adherence to a “PRISM day” criterion. A PRISM day is defined as 1200 UTC-1200 UTC (e.g., 7 AM-7AM EST), which is the same as the [the National Weather Service’s hydrologic day]. Once-per day observation times must fall within +/- 4 hours of the PRISM day to be included in the AN81d tmax and tmin datasets. Stations without reported observation times in the NCEI GHCN-D database are currently assumed to adhere to the PRISM day criterion. The dataset uses a day-ending naming convention, e.g., a day ending at 1200 UTC on 1 January is labeled 1 January.\n\nThis definition means that generally only morning observers should be included in the data. The last sentence is important: because a day runs from 4am-4am PST (or 7am-7am EST) and because days are labeled using the endpoint of that time period, most of the observations from which the daily measures are constructed for a given date are taken from the day prior. A diagram may be helpful here:\n\n\n\nDiagram\n\n\nThe above is a plot of temperature over about two days, representing a possible set of within-day monitor data. Let’s say that this station takes a morning reading at 7am PST (10am EST), meaning that this station would be included in the PRISM dataset. The top x-axis is the actual date, while the bottom x axis shows which observations are used under the PRISM day definition. The red lines are actual midnights, the dark green dotted line is the PRISM day definition cutoff and the orange (blue) dots in the diagram are the observations that represent the true maximums (minimums) of that calendar day. Because of the definition of a PRISM day, the maximum temperatures (“tmax”s from here on out) given for Tuesday and Wednesday (in PRISM) are actually observations recorded on Monday and Tuesday, respectively. On the other hand, the minimum temperatures (“tmin”s) given for Tuesday (in PRISM) is actually drawn from Tuesday, but the tmin given for Wednesday (in PRISM) is also from Tuesday.\nTo see this visually, I pulled the GHCN data and plotted a histogram of the average reporting time by station for the stations that report observation time (66% in the United States). The histogram below shows the average observation time by stations for all GHCN-D stations in the continental United States in UTC, colored by whether or not they would be included in PRISM according to the guidelines given above.\n\n\n\nHistogram of observation time\n\n\nThis confirms what I asserted above: most, but not all, GHCN-D stations are morning observers, and the PRISM day definition does a good job capturing the bulk of that distribution. On average, stations fulfilling the PRISM criterion report at 7:25am or so.\nThe next step is to look empirically at how many minimum and maximum temperature observations are likely to fall before or after the observation time cutoff. To do that, we need some raw weather station data, which I pulled from NOAA’s Quality Controlled Local Climatological Data (QCLCD). To get a sense for which extreme temperatures would be reported as occurring on the actual day they occurred, I assumed that all stations would report at 7:25am, the average observation time in the PRISM dataset. The next two figures show histograms of observed maximum and minimum temperatures.\n \nI’ve colored the histograms so that all extremes (tmins and tmaxes) after 7:25am are red, indicating that extremes after that time will be reported as occurring during the following day. As expected, the vast majority of tmaxes (&gt;94%) occur after 7:25am. But surprisingly, a good portion (32%) of tmins do as well. If you’re concerned about the large number of minimum temperature observations around midnight, remember that a midnight-to-midnight summary is likely to have this sort of “bump”, since days with a warmer-than-usual morning and a colder-than-usual night will have their lowest temperature at the end of the calendar day.\nAs a more direct check, I compared PRISM leads of tmin and tmax to daily aggregates (that I computed using a local calendar date definition) of the raw QCLCD data described above. The table below shows the pairwise correlations between the PRISM day-of observations, leads (next day), and the QCLCD data for both maximum and minimum daily temperature.\n\n\n\nMeasure\nPRISM day-of\nPRISM lead\n\n\n\n\ntmin (calendar)\n0.962\n0.978\n\n\ntmax (calendar)\n0.934\n0.992\n\n\n\nAs you can see, the the PRISM leads, i.e., observations from the next day, correlated more strongly with my aggregated data. The difference was substantial for tmax, as expected. The result for tmin is surprising: it also correlates more strongly with the PRISM tmin lead. I’m not quite sure what to make of this - it may be that the stations who fail to report their observation times and the occasions when the minimum temperature occurs after the station observation time are combining to make the lead of tmin correlate more closely with the local daily summaries I’ve computed. But I’d love to hear other explanations.\nSo who should be concerned about this? Mostly, researchers with econometric models that use daily variation in temperature on the right-hand side, and fairly high frequency variables on the left-hand side. The PRISM group isn’t doing anything wrong, and I’m sure that folks who specialize in working with weather datasets are very familiar with this particular limitation. Their definition matches a widely used definition of how to appropriately summarize daily weather observations, and presumably they’ve thought carefully about the consequences of this definition and of including more data from stations who don’t report their observation times. But researchers who, like me, are not specialists in using meteorological data and who, like me, use PRISM to examine at daily relationships between weather and economics outcomes, should tread carefully.\nAs is, using the PRISM daily tmax data amounts to estimating a model that includes lagged rather than day-of temperature. A quick fix, particularly for models that include only maximum temperature, is to simply use the leads, or the observed weather for the next day, since it will almost always reflect the maximum temperature for the day of interest. A less-quick fix is to make use of the whole distribution using the raw monitor data, but then you would lose the nice gridded quality of the PRISM data. Models with average or minimum temperature should, at the very least, tested for robustness with the lead values. Let’s all do Gibran proud."
  },
  {
    "objectID": "posts/2018-09-14-rstudio-debugging/2018-09-14-rstudio-debugging.html",
    "href": "posts/2018-09-14-rstudio-debugging/2018-09-14-rstudio-debugging.html",
    "title": "Debugging in R, RStudio",
    "section": "",
    "text": "Debugging can be a challenge in RStudio. One of my main frustrations is that once you execute the Run command on a selection of code (i.e., running it in interactive mode), it will send all the commands you selected, even if one or more of them raises an error. This often results in me running a full script, even after an error occurs on one of the first few lines of code. In most cases, nothing remaining in the script can run successfully without whatever errored out earlier. For example, suppose I’m executing the following code:\nx &lt;- 4\nz &lt;- x + y\nprint(z)\nx &lt;- x^2\nSys.sleep(5000) # Placeholder for a time-consuming command\nBecause I forgot to define y before calling it, the second command fails with an error. But because I’m running this interactively, the third command runs (and fails) as well. The fourth command then runs successfully, modifying x. This introduces a new problem: I can no longer easily fix the problem by fixing the second command and rerunning the subsequent commands – I have to either start from the beginning or manually fix the modified variables. Finally, the fifth command runs and hangs the R console until I hit stop. If this was a command that was loading a large dataset (as it often is), the only way I can stop it is by killing R and restarting.\nUnfortunately, there’s no way to solve this in interactive mode – RStudio seems to be committed to running everything you send, no matter what. The best solution is to instead source the document (CTRL-S in RStudio), which runs the entire script from the beginning. This is fine if you don’t have a bunch of data you need to load initially. But since I often do, I have to comment out everything I don’t want re-run.\nOne alternative to this is encasing everything in a function, since functions do halt on errors. This makes debugging and development challenging (despite the browser() command), so I view this as a less-useful solution.\nI’m still thinking this over, but here are a general set of principles for efficient debugging in a research context: 1. Minimize overall execution time whenever possible – sample from datasets and save intermediary datasets that are time-consuming to compile. 2. If the above is not possible, run from source (commenting higher up portions as needed - this can be made easier with intermediary datasets) instead of interactively. 3. Functionalize as much as possible."
  },
  {
    "objectID": "posts/2015-09-08-researchproductivity/2015-09-08-researchproductivity.html",
    "href": "posts/2015-09-08-researchproductivity/2015-09-08-researchproductivity.html",
    "title": "Productivity and organization notes",
    "section": "",
    "text": "Warning\n\n\n\nDecember 2022: I’ve left this post up for posterity, but I habitually reinvent my work process every six months or soo, so at this point these notes are about well out of date. My philosophy around “productivity” continues to evolve, as does the set of tools I use.\n\n\n\nWork process notes\n\nBeginning of arrival in new space: 30 minutes of uninterrupted work\nNo e-mail, no paying bills, definitely no surfing the internet. Momentum is important!\nAfter the frenzy, 25 min on, 5 min off (pomodoro)\nLong-term schedule sketches out broad goals. In general maybe for a semester or something, right now focus on JMP.\nShort-term schedule made at the beginning of every week. Sketch out morning and afternoon activities.\nAt end of workday, write down what I accomplished in a work journal. It’d be really nice to do this in the same place where the schedule is, somehow\n\n\n\nDropbox organization\nWinter break is a good time to sit back, reflect on the year, and… reorganize your Dropbox folder. If you’re the kind of semi-compulsive organizer that I am, that is. After stumbling upon Michael Descy’s Plaintext Productivity, the notion to reorganize my work documents and habits has been stealthily invading my brain. I don’t need a system as complex as Descy, but I do need a system. I’ve made two major changes:\n\nConverted my note-taking process and most informal writing into markdown. Markdown is, in some ways, an incomplete replacement for org-mode (my previous note-taking methodology) in emacs, but with SmartMarkdown in Sublime Text 3, I can at least fold up the sections. Which is 90% of what I was missing from org-mode. Markdown is nice because it can be easily edited with any editor in any operating system, looks almost exactly like plaintext to most people, and can be used to VERY easily generate HTML. Also, a good deal of my work is already in Sublime Text and while knowing emacs did hold a certain cachet, I don’t have a strong desire to keep two distinct sets of keyboard shortcuts in mind. Also emacs loads more slowly. Also lisp (and elisp) drive me crazy.\nRestructured my Dropbox folder.\n\nI had the following requirements: 1. Quick, logical access to data/work 2. Pain-free archiving 3. Incorporate school and work into Dropbox 4. Deal elegantly with non-Dropbox files (too big for Dropbox)\nTo do this, I’ve restructured my Dropbox folder as follows:\n\n01_Work\n\n01_Current: Current projects\n\nProjects on which I am an author in base directory\nWork for others in subdirectories\n\ne.g. 01_Current/Joe/Project A\n\n\n02_Archive: Archived projects by year most recently updated\n\nSame structure as Current projects\ne.g. 02_Archive/2013/Joe/…\n\n03_Ideas: Project ideas\n\nNascent ideas that have some associated documents in them in base directory\n\nOnce they’re more developed, move to 01_Current\n\nOnce ideas are stale, move to subdirectory Archive/… (no year necessary)\n\n04_Notes\n\nIncludes notes like, work.md, and so on\nToolbox: math, econometrics, etc. handouts that I’ve downloaded and want to save\n\n\n02_Personal\n\nEverything from my non-work life\n\n\nIf I don’t break Dropbox, this will be great. The only requirement this doesn’t capture is #4. I’ve decided the best thing (for now) is to just include shortcuts to the data folders. Since almost all of these are my bigger machine MONDO, it’s an issue I only need to deal with there."
  },
  {
    "objectID": "posts/2019-06-11-making-regressions-purrr/2019-06-11-making-regressions-purrr.html",
    "href": "posts/2019-06-11-making-regressions-purrr/2019-06-11-making-regressions-purrr.html",
    "title": "Making regressions purrr",
    "section": "",
    "text": "I often need to run multiple sets of regressions on the same or similar datasets. This is usually for some set of robustness checks, either to help me better understand the stability of some result or to respond to referee requests. For years, this has been a mostly ad-hoc process for me. First, I would just copy-paste my regressions, modifying one variable or filter on the dataset with each paste. When this got to be too manual, I turned to nested loops and/or apply functions. This was an improvement in terms of running the regressions in a more systematic way, but extracting results I wanted to look at or highlight easily wasn’t straightforward. However, the purrr package (part of the tidyverse) provides tools that can make all of this easier.\nThe following code, along with a couple functions I’ve added to baylisR (call it a vanity package), allows me to facilitate a few common tasks:\n\nI can easily build a set of regressions I want to run by combining different possible variables and datasets.\nThe output can be saved compactly as a tibble with a list-column containing either a stripped-down version of the fitted felm object, or a tidied data.frame of the same.\nI can easily select specific statistical specifications for display in tables or figures.\n\nThere’s a bit more beneath the hood of this code. You’ll want to refer to the code of reg_felm to see how to call it. If you’re going to implement this yourself, I recommend you don’t rely on baylisR but instead extract the code for reg_felm (and strip_felm, which it calls) and modify it for your own purposes.\n\npacman::p_load(tidyverse, huxtable, stargazer, plm)\n\n# Load data\ndata(\"Wages\", package = \"plm\")\n\n# Convenience function, fits model use lfe::felm following inputs\nreg_felm &lt;- function(lhs, rhs, fe = \"0\", iv = \"0\", clus = \"0\", data_str) {\n    data &lt;- eval(parse(text = data_str))\n    fmla &lt;- sprintf(\"%s ~ %s | %s | %s | %s\", lhs, rhs, fe, iv, clus)\n    fit &lt;- lfe::felm(as.formula(fmla), data = data)\n}\n\n\nreg_tibble &lt;- as_tibble(\n    expand.grid(lhs = \"lwage\", rhs = c(\"exp\", \"exp + wks\"), \n                fe = \"married\", clus = \"0\", data_str = c(\"Wages\", \"Wages %&gt;% filter(bluecol == 'no')\"),\n                stringsAsFactors = F))\n\nreg_tibble$fit &lt;- purrr::pmap(reg_tibble, reg_felm)\n\n\n# These can be used directly in stargazer or huxtable...\nstargazer::stargazer(reg_tibble$fit)\n\n\n% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com\n% Date and time: Sat, Dec 10, 2022 - 23:35:58\n\\begin{table}[!htbp] \\centering \n  \\caption{} \n  \\label{} \n\\begin{tabular}{@{\\extracolsep{5pt}}lcccc} \n\\\\[-1.8ex]\\hline \n\\hline \\\\[-1.8ex] \n & \\multicolumn{4}{c}{\\textit{Dependent variable:}} \\\\ \n\\cline{2-5} \n\\\\[-1.8ex] & \\multicolumn{4}{c}{fmla} \\\\ \n\\\\[-1.8ex] & (1) & (2) & (3) & (4)\\\\ \n\\hline \\\\[-1.8ex] \n exp & 0.007$^{***}$ & 0.007$^{***}$ & 0.012$^{***}$ & 0.012$^{***}$ \\\\ \n  & (0.001) & (0.001) & (0.001) & (0.001) \\\\ \n  & & & & \\\\ \n wks &  & 0.004$^{***}$ &  & 0.008$^{***}$ \\\\ \n  &  & (0.001) &  & (0.002) \\\\ \n  & & & & \\\\ \n\\hline \\\\[-1.8ex] \nObservations & 4,165 & 4,165 & 2,036 & 2,036 \\\\ \nR$^{2}$ & 0.110 & 0.112 & 0.160 & 0.166 \\\\ \nAdjusted R$^{2}$ & 0.110 & 0.112 & 0.159 & 0.165 \\\\ \nResidual Std. Error & 0.435 (df = 4162) & 0.435 (df = 4161) & 0.415 (df = 2033) & 0.414 (df = 2032) \\\\ \n\\hline \n\\hline \\\\[-1.8ex] \n\\textit{Note:}  & \\multicolumn{4}{r}{$^{*}$p$&lt;$0.1; $^{**}$p$&lt;$0.05; $^{***}$p$&lt;$0.01} \\\\ \n\\end{tabular} \n\\end{table} \n\nhuxtable::huxreg(reg_tibble$fit)\n\n\n\n\n\n(1)\n(2)\n(3)\n(4)\n\n\nexp\n0.007 ***\n0.007 ***\n0.012 ***\n0.012 ***\n\n\n\n(0.001)   \n(0.001)   \n(0.001)   \n(0.001)   \n\n\nwks\n        \n0.004 ** \n        \n0.008 ***\n\n\n\n        \n(0.001)   \n        \n(0.002)   \n\n\nN\n4165        \n4165        \n2036        \n2036        \n\n\nR2\n0.110    \n0.112    \n0.160    \n0.166    \n\n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\n\n\n# ...Or shown all together in a single table, after tidying.\nbind_rows(lapply(reg_tibble$fit, broom::tidy), .id = \"reg_id\")\n\n\n\n\nreg_id\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n1\nexp\n0.00705\n0.000624\n11.3 \n3.32e-29\n\n\n2\nexp\n0.00714\n0.000623\n11.5 \n6.39e-30\n\n\n2\nwks\n0.00433\n0.00132 \n3.29\n0.00102 \n\n\n3\nexp\n0.0116 \n0.000889\n13.1 \n1.26e-37\n\n\n4\nexp\n0.0117 \n0.000886\n13.2 \n3.71e-38\n\n\n4\nwks\n0.0079 \n0.00195 \n4.05\n5.26e-05\n\n\n\n\n\n\n# It's also straightforward to display only a subset of the regressions.\nhuxtable::huxreg(reg_tibble %&gt;% filter(rhs == \"exp + wks\") %&gt;% pull(fit))\n\n\n\n\n\n(1)\n(2)\n\n\nexp\n0.007 ***\n0.012 ***\n\n\n\n(0.001)   \n(0.001)   \n\n\nwks\n0.004 ** \n0.008 ***\n\n\n\n(0.001)   \n(0.002)   \n\n\nN\n4165        \n2036        \n\n\nR2\n0.112    \n0.166    \n\n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05."
  },
  {
    "objectID": "posts/2015-10-12-conley-se-and-fe/2015-10-12-conley-se-and-fe.html",
    "href": "posts/2015-10-12-conley-se-and-fe/2015-10-12-conley-se-and-fe.html",
    "title": "Conley standard errors and high dimensional fixed effects",
    "section": "",
    "text": "Warning\n\n\n\nUpdate (April 2022): I’ve left this post up for posterity, but, full disclosure, the “solution” I offer below the break isn’t much of a solution at all. These days, when I have a project with this kind of potential issue, I usually cluster the standard errors by a large administrative unit (e.g., state) as a way of ensuring that inference isn’t driven by spatial correlation. This is very likely less efficient than a spatial clustering approach, however.\nSo how would I do this now, in 2022? It turns out that the endlessly impressive R package fixest (developed by Laurent Berge) now has a routine (search for “conley”) for it. So that would be my first stop. If I were still a Stata user, I would look into the acreg command.\nIf anyone has an alternative, better method for computing Conley SEs, please do let me know. And if you still want to check out the old post, it’s below the break.\n\n\nOriginal blog post (2015): For my JMP, I cluster my standard errors two ways, across both geographies and time. During a recent seminar, one of the audience members asked me why I wasn’t using spatial standard errors instead, for example those described in Conley (2008).\nA case where this might matter is as follows: suppose I’m worried about correlated variation in both my left- and right-hand side variables between observations that are near each other (putting aside correlation across time for now since the concept is equivalent). One typical solution, equivalent to what I was using, is to cluster at some geographic level, say by county. If the correlations only occur within each county, then this is sufficient. If, however, observations across county lines are correlated (e.g. Kansas City, then the standard errors I estimate may be too small. Conley standard errors solve this problem. In fact, one of my advisers, Sol Hsiang, implements these errors for both Matlab and Stata. I also found a version for R, though I haven’t tested it.\nHowever, I have a lot of data and multiple dimensions of fixed effects, so I am using Sergio Correia’s fantastic reghdfe Stata package. He is planning to implement Conley standard errors, but hasn’t gotten around to it yet. Thiemo Fetzer implements a solution using both Sol’s code, but it uses reg2hdfe (which is similar, but generally slower than reghdfe) and looks complicated.\nInstead, I use hdfe, which does the demeaning for reghdfe, to partial out my fixed effects. Then I run Sol’s code on the demeaned data. I’ve posted the code without context below, to give an example:\nhdfe afinn_score_std tmean_b*, clear absorb(gridNum statemonth) \\\\\\\n    tol(0.001) keepvars(gridNum date lat lng)\n\nols_spatial_HAC afinn_score_std tmean_b*, lat(lat) lon(lng) \\\\\\\n    time(date) panel(gridNum) distcutoff(16) lagcutoff(7) disp\nThis appears to be much too slow for my dataset (&gt;2 million obs at different locations, more than a year of daily data). I found an updated version in R from Thiemo Fetzer again, but this too isn’t fast enough for my needs, even though the matrix operations are implemented in C++. For now, the best solution is to estimate these errors on a random subset of the data."
  },
  {
    "objectID": "posts/2019-12-05-scoring-texts/2019-12-05-scoring-texts.html",
    "href": "posts/2019-12-05-scoring-texts/2019-12-05-scoring-texts.html",
    "title": "Scoring texts for the presence of phrases",
    "section": "",
    "text": "In my text analysis work, I frequently score texts for the presence or absence of various ``keywords’’. Because I work with some large corpora (collections of texts), for example the billions of tweets in my job market paper, this can be a time-consuming task. I have previously done most of this in Python, but right now I’m also interested in doing it quickly in R for ad hoc analyses.\nIn this post I test two different methods for detecting words: a simpler `grepl’-based approach that using a regex search to identify any texts with at least one of the matching words, and slightly more involved approach first tokenizes (i.e., breaks up sentences into their component word ‘tokens’) the texts and then searches through those tokens to see if any match the given keywords.\n\nlibrary(tidyverse)\n\n# Load 50 sample tweets\nexample_tweets &lt;- read_csv(\"tweet-examples.csv\") %&gt;% pull(tweet50) \n# Simulate a corpus of 250,000 tweets\ncorpus &lt;- rep(example_tweets, 5000) \n\n# Create the vector of ten words to search for\nkeywords &lt;- c(\"money\", \"beer\", \"friends\", \"fhdjasl\", \"dfjha\", \"dshjfsa\", \"fdjh\", \"vjhd\", \"cvmna\", \"dfhda\")\n\n# Score using grepl approach ----\n# We only want to match on distinct words, so we use the word boundaries approach and paste all of the words together with |\nscore_grepl &lt;- function(texts, keywords) {\n    sapply(texts, function(x) grepl(sprintf(\"\\\\b(%s)\\\\b\", paste0(keywords, collapse = \"|\")), x), USE.NAMES = F)\n}\n\nsystem.time(score_grepl(corpus, keywords[1])) # About 2.85s\nsystem.time(score_grepl(corpus, keywords)) # About 9.5s\nsystem.time(score_grepl(corpus, rep(keywords, 10))) # About 120s\n\n# Score with tokenizer appraoch using tokenizers package ----\nlibrary(tokenizers)\n# Note that this is a bit more sophisiticated than just using grepl, since we ask it to strip out punctuation and urls.\nscore_tokenizers &lt;- function(texts, keywords) {\n    tkns &lt;- tokenize_tweets(texts, strip_punct = TRUE, strip_url = T)\n    sapply(tkns, function(x) any(keywords %in% x), USE.NAMES = F)\n}\n\nsystem.time(score_tokenizers(corpus, keywords[1])) # About 5s\nsystem.time(score_tokenizers(corpus, keywords)) # About 5s\nsystem.time(score_tokenizers(corpus, rep(keywords, 10))) # About 6s\n\nWhat did I learn here? If I’m only interested in detecting a small number of keywords, a simple grepl based approach is fine. But if I want to search for more than 5-10 keywords, tokenizing first is better (I also test a tokenizing approach that uses the quanteda package, but do not see any notable differences in performance). Note that this would also be true for creating multiple scores from the same set of texts."
  },
  {
    "objectID": "posts/2021-01-22-predict-partial/2021-01-22-predict-partial.html",
    "href": "posts/2021-01-22-predict-partial/2021-01-22-predict-partial.html",
    "title": "Partial predictions",
    "section": "",
    "text": "In climate economics and in other settings, we often would like to estimate a response function, or the outcome as a function of some covariate, i.e., \\(y = f(T)\\). Most of the time, \\(T\\) stands for temperature. Figure 3 in Carleton and Hsiang (2016) documents a bunch of different response functions from the literature.\nThese models, however, usually include lots of controls, including fixed effects. Consider the following (fairly generic) estimating equation:\n\\[y_{it} = f(T; \\beta) + \\phi_i + \\varepsilon_{it}\\] This model includes unit fixed effects (\\(\\phi_i\\) and \\(\\phi_t\\)}). We include these usually it’s only reasonable to interpret the relationship between \\(y\\) and \\(f(T)\\) as causal if we have accounted for potential confounding factors in the cross-sectional and time-series relationships between climate and the outcome. Put another way, it wouldn’t make much sense to attribute the difference between the GDP per capita of Switzerland and the GDP per capita of Mexico to the differences in climate alone, so instead we control for those countries’ average characteristics using fixed effects and estimate the causal relationship using only residual variation in temperature and the outcome. That is to say, we zoom in on years when Switzerland was, say, exceptionally warm and compare its GDP per capita to years when it was exceptionally cold, and then do the same for Mexico.\nComputationally, this could mean including a large set of fixed effects, so estimating these models via ordinary least-squares (OLS) using dummy variables is a nonstarter for settings with thousands of fixed effects, computationally speaking. Instead, we use programs such as fixest (there are many others, but fixest is the best one I know of, right now), which use fancy mathematical tricks to alleviate the computational burden.\nOne of the most intuitive ways to consider the relationships we estimate, particularly when \\(f(T)\\) is non-linear, is to just plot it, with standard errors included. That’s where we run into some issues. Unlike the base R function lm, fixest does not have a native prediction function that can supply standard errors. This is for a good reason: strictly speaking, predicting the outcome \\(y\\) requires that we observe the variance-covariance matrix (call it the “vcov”) for all of the terms, including the fixed effects. But one of the side effects of the mathematical tricks we use is that we can no longer obtain the full vcov.\nSo, we’re basically sunk if we want to plot \\(\\hat{y}\\) with standard errors included. But what if we’re just interested in the relationship between \\(T\\) and \\(y\\), holding all the other controls constant? I.e., what if we just want to plot the response function \\(f(T)\\)? In that case, and assuming a linear-in-parameters model1, we only need the vcov for the coefficients related to the variable we’re allowing to change, in this case \\(T\\).\nTo see how this works, consider that a response function is really just the expected value of the outcome across a range of \\(T\\) (say, 0 to 40) minus \\(\\hat{y}\\) at a single value of \\(T\\) (say, 20 C). That is, for T = 40, the expected value of\n\\[E[y_{it}|T=40] - E[y_{it}|T = 20] = f(T=40; \\hat{\\beta}) + \\phi_i - f(T=20; \\hat{\\beta}) - \\phi_i = f(T=40; \\hat{\\beta}) - f(T=20; \\hat{\\beta})\\]\nAs you can see, the fixed effects drop out, so for this linear-in-parameters model at least we don’t need the entire vcov to generate standard errors for each point on the response curve. So far as I know, though, there’s no easy way to run this kind of prediction without a bit of wrangling. Typically, I’ve used survey::svycontrast to do this sort of thing (see here), but it always felt a bit fiddly. So the following code lets us do this kind of partial prediction easily, by just passing the variables we want to predict over.2\nlibrary(fixest)\nlibrary(ggplot2)\nlibrary(cowplot)\n\npredict_partial &lt;- function(object, newdata, se.fit = FALSE,\n                            interval = \"none\",\n                            level = 0.95){\n    if(missing(newdata)) {\n        stop(\"predict_partial requires newdata and predicts for all group effects = 0.\")\n    }\n    \n    object &lt;- fit_feols; newdata &lt;- newdata; se.fit = T; interval = \"confidence\"; level = 0.95\n    \n    # Extract terms object, removing response variable \n    tt &lt;- terms(object)\n    Terms &lt;- delete.response(tt)\n    \n    # Remove intercept\n    attr(Terms, \"intercept\") &lt;- 0\n    \n    X &lt;- model.matrix(Terms, data = newdata)\n    \n    if (class(object) == \"fixest\") {\n        B &lt;- as.numeric(coef(object))\n        df &lt;- attributes(vcov(fit_feols, attr = T))$dof.K\n    } else if (class(object) %in% c(\"lm\", \"felm\")) { \n        B &lt;- as.numeric(object$coef)\n        df &lt;- object$df.residual\n    } else {\n        stop(\"class(object) should be lm, fixest, or felm.\")\n    }\n    \n    fit &lt;- data.frame(fit = as.vector(X %*% B))\n    \n    if(se.fit | interval != \"none\") {\n        sig &lt;- vcov(object)\n        se &lt;- apply(X, MARGIN = 1, FUN = get_se, sig = sig)\n    }\n    \n    if(interval == \"confidence\"){\n        t_val &lt;- qt((1 - level) / 2 + level, df = df)\n        fit$lwr &lt;- fit$fit - t_val * se\n        fit$upr &lt;- fit$fit + t_val * se\n    } else if (interval == \"prediction\"){\n        stop(\"interval = \\\"prediction\\\" not yet implemented\")\n    }\n    if(se.fit){\n        return(list(fit=fit, se.fit = se))\n    } else {\n        return(fit)\n    }\n}\n\nget_se &lt;- function(r, sig) {\n    # Compute linear combination, helper function for predict_partial\n    # Given numeric vector r (the constants) and vcov sig (the ), compute SE \n    r &lt;- matrix(r, nrow = 1)\n    sqrt(r %*% sig %*% t(r))\n}\n\nN &lt;- 100\ndata &lt;- data.frame(x = rnorm(N), group = rep(c(\"A\", \"B\"), times = N/2))\ndata$y &lt;- 1 + 3 * data$x - 2 * data$x^2 + rnorm(N, 0, 0.5) + as.numeric(data$group == \"A\") * 3\n\nfit_feols &lt;- feols(y ~ x + I(x^2) | group, data = data)\n\nnewdata &lt;- data.frame(x = seq(-3, 3, 0.1))\npreds &lt;- predict_partial(fit_feols, newdata, se.fit = T, interval = \"confidence\")\n\nplot_df &lt;- cbind(newdata, preds$fit)\n\nggplot(plot_df, aes(x = x, y = fit)) + \n    geom_line() +\n    geom_line(aes(y = lwr), linetype = \"dashed\") + \n    geom_line(aes(y = upr), linetype = \"dashed\") + \n    theme_cowplot()\nNote that this function also works with lm and felm, though there’s little reason to use it for lm given that predict.lm works (and provides more functionality).\nfit_lm &lt;- lm(y ~ x + I(x^2), data = data)\npreds_lm &lt;- predict_partial(fit_lm, newdata, se.fit = T, interval = \"confidence\")\nlibrary(lfe)\n\nLoading required package: Matrix\n\nfit_felm &lt;- felm(y ~ x + I(x^2), data = data)\npreds_felm &lt;- predict_partial(fit_felm, newdata, se.fit = T, interval = \"confidence\")\nIn any case, I’m happy to finally close this issue, given that it’s been on my mind for almost six years."
  },
  {
    "objectID": "posts/2020-08-21-remote-teaching/2020-08-21-remote-teaching.html",
    "href": "posts/2020-08-21-remote-teaching/2020-08-21-remote-teaching.html",
    "title": "How I’m remote teaching a big class this fall",
    "section": "",
    "text": "I start teaching remotely in two weeks. Helping 140 students spread across the world (so many timezones!) learn about environmental economics is a daunting task. Fortunately, I have great colleagues with good ideas about how to do it. I canvassed my network on Twitter yesterday, starting with what I had planned for the course:\n\n\n\n{{% tweet \"1296525738095648772\" %}}\n\n\n\nThis short post led a wide-ranging thread with lots of great input from people who have thought more deeply about this. Happily, many others are following a similar mixed delivery model, where required content is available anytime, but students can also meet with me and/or their classmates in real time. But there were a few great tweaks they suggested. Having digested their input, here’s the plan for the fall (also planning to post the syllabus once I’ve finished updating it):\n\nPre-recorded video lectures, 5-8 minutes each.\n\nI originally planned for 10-15 minutes, but @JenniferRaynor_ and @hagertynw convinced me to try to go shorter.\n@RichardTol has made an amazing set of videos (his and others) available here.\n\nWeekly discussion quizzes and problem sets\n\n@MarinaAdshade’s comment (below) convinced me that it’s better to have more, lower-risk assessments, so I’m trying to figure out how to work that into the course without adding too much load for the TA.\nRight now, I am leaning towards keeping the quizzes completion-only but adding one or two more problem sets (for a total of five throughout the term). Alternatively, I could drop the problem sets entirely and incorporate their content into the discussion quizzes, with some questions assessed for completion only and some for correctness.\n@mattsclancy pointed me towards LearnItFast.io, which I like the look of but won’t be able to get around to this year. See his thread for more.\n\nWeekly meeting for workshopping & questions\n\n@mrobj is planning to use this time for breakout rooms and experiments. That’s an endeavor for another year for me, but feels like an exciting idea that could work even in non-pandemic years.\n\nOpen note, time-limited exams\n\nA couple folks raised reasonable concerns about the possibility of cheating in this format.\n@MarinaAdshade commented that she is moving more towards low-stakes assessments rather than a small number of high risk exams as a way to minimize the incentives to cheat.\n@npmagnan is doing open-note exams with randomized question and answer order, plus no-backtracking. Students don’t like the no-backtracking rule but it limits the ability to “collaborate”.\nThis year, I’ve decided to take a middle ground: I’m dropping the midterm and keeping the final, while loading more weight onto the weekly discussion quizzes (see above).\n\n\nThere’s a lot left to do, but I’m excited for this year for two reasons. First, this change in approach is much better than just transferring my usual in-person song and dance to Zoom. It’s also more humane for students living in distant timezones. I know it’s going to be hard for students to stay engaged online no matter what, but it’s got to be better than, say, watching me drone on for 80 minutes straight. At the same time, having synchronous sessions available will help the students get their questions answered and hopefully provide some measure of much-needed social interaction. Finally, more low-risk assessments will hopefully help students feel like they don’t need to cheat to succeed in the class.\nSecond, I think a lot of the additional work I put in for this course will also pay off in future, (hopefully) non-pandemic years. With a set of digestible lecture videos, I can move towards more of a flipped classroom approach. Plus, having learned how to use OBS Studio to record lectures, doing so in the future will be much easier (more on the recording setup later). And since more frequent, lower risk quizzes will lower the pressure of assessments in an in-person setting as well, I hope to continue that into the future as well. Doing so will also let me implement spaced repetition to help students retain information better.\nThanks to everyone, including those not mentioned above, who contributed to the discussion yesterday. As the Wisconsin motto says… Forward!"
  },
  {
    "objectID": "posts/2021-08-15-pop-weighted-weather/2021-08-15-pop-weighted-weather.html",
    "href": "posts/2021-08-15-pop-weighted-weather/2021-08-15-pop-weighted-weather.html",
    "title": "Pop-weighted averages from rasters",
    "section": "",
    "text": "Note\n\n\n\nUpdated February 10, 2022 to incorporate prism package.\n\n\nMuch of my work examines how the environment impacts people, often at a national scale. As a result, I often end up using environmental data that is packaged as a raster (i.e., a bunch of grid cells laid over the earth) that I would like to match with adminstrative shapes (e.g., provinces or counties) that are packaged as polygons. Moreover, I often want to do that matching in a way that respects the distribution of population.\nTo give a more specific example, I often end up (gratefully) using weather data from the PRISM Climate Group at Oregon State, who make rasters of daily and monthly weather data available for the entire United States. The basic idea behind the PRISM approach is that they interpolate weather station data in a sophisticated way that accounts for weather-relevant topographic features, e.g., elevation (details here if that’s your thing). This is usually an improvement over more naive approaches that simply take some sort of distance-weighted average.\nSo PRISM is really good for generating highly-resolved measurements of weather, and their datasets are packaged as a set of rasters (in fact, there’s even an R package that will help download and manage the PRISM data). This goal of this post is to provide an example for how to get from that raster data to population-weighted county averages, which you can then use in an analysis.\n\nWhy should I weight by population?\nTo see why you should think about weighting by population, let’s take an example from the 1991 cult classic Point Break, which remains one of my favorite movies. If you haven’t seen it, Point Break is a movie about surfing, crime, and the meaning of life, and it stars Johnny Utah (played by Keanu Reeves) and Bodhi (played by Patrick Swayze, RIP).\nSo, imagine you’re looking down at the County of Los Angeles, much as Johnny and Bodhi do as they parachute down on top of it.\n\n\n\nBodhi: “You’re about to jump out a perfectly good airplane, Johnny! How do you feel about that?”\n\n\nLooking down, Johnny and Bodhi would obviously (obviously!) notice first that the population of Los Angeles County is not distributed evenly across the landscape: LA, especially downtown, is of course densely populated, but many other areas, like the mountains and various beach towns that dot its coastline, have far fewer numbers of people per unit of land area. So from the perspective of observing the weather experienced by average person in LA experiences, an unweighted average isn’t strictly correct. What we really want is some sort of population-weighted average.\nTaking population-weighted averages is one of those small things that almost always improves the fit of any statistical model I estimate, since most of the time I’m interested in people’s responses to weather (or controlling for weather), and for that reason it just makes more sense to derive measures that better represent the weather experienced by the actual humans on the ground. It used to be a fairly significant task to compute population-weighted averages, but these days there are tools in R that make it much easier.\nIn the tutorial that follows, we’ll lean heavily on exactextractr to do the weighting. Our goal will be to generate a population-weighted average of the weather on July 12, 1991 for every county in California. I’m sure I don’t have to tell anyone this, but that’s the date that Point Break was released.\n\n\nPreliminaries\nOur first task is to load a bunch of packages. Besides exactextractr, we’ll load many of the usual suspects.\nNext, we want to download a shapefile with our output geography (counties, in this case) and load the PRISM weather data and the population data. The last two are both rasters, and I’ve included their sources in the code. You’ll still need to download and unzip them to your project directory, though.\n\n# Load CA counties\nCA_counties = tigris::counties(state = \"CA\", cb = TRUE, resolution = \"20m\")\n\n# Load PRISM raster\nprism_set_dl_dir(\"./\")\nget_prism_dailys(type = \"tmean\", minDate = \"1991-07-12\", maxDate = \"1991-07-12\", keepZip = F)\nweather_rast = prism_archive_ls()[1] %&gt;% pd_to_file %&gt;% raster\n# Note that the prism package has a bunch of convenience functions, \n# but in this case we only need one raster\n\n# Load Population raster\n# SOURCE: https://sedac.ciesin.columbia.edu/data/set/usgrid-summary-file1-2000/data-download\n# Register/Log in, select Census 2000, population for CA, download and unzip\npop_rast = raster(\"usgrid_data_2000/ascii/capop00.asc\")\n\n\n\nVisualizing the rasters\nNow that we have these loaded, let’s do some visualization of what we’re working with. We’ll focus on just Los Angeles County for now, since that will let us see the (fine) weather grid cells, as well as the even finer population rasters.\n\n# Select LA county\nLA_county = CA_counties %&gt;% filter(NAME == \"Los Angeles\")\n\n# Crop both rasters to LA county and convert to a data.frames (just for plotting)\nweather_crop = crop(weather_rast, LA_county)\nweather_df = as.data.frame(weather_crop, xy = TRUE)\n\npop_crop = crop(pop_rast, LA_county)\npop_df = as.data.frame(pop_crop, xy = TRUE)\n\n# Plot shapefile and raster boundaries\nggplot(LA_county) + \n    geom_sf() + \n    geom_tile(data = weather_df, aes(x = x, y = y), fill = NA, colour = \"black\", size = 0.05, alpha = 0.5) +  \n    geom_tile(data = pop_df, aes(x = x, y = y), fill = NA, colour = \"red\", size = 0.05, alpha = 0.5) +  \n    theme_map()\n\n\n\n\n\n\n\n\nIt’s a little hard to see what’s happening with the grid cells, so let’s zoom into a location at random, like Neptune Net, the restaurant where Johnny Utah first approaches Tyler (played by Lori Petty) about learning to surf.\n\nggplot(LA_county) + \n    geom_sf() + \n    geom_tile(data = weather_df, aes(x = x, y = y), fill = NA, colour = \"black\") +  \n    geom_tile(data = pop_df, aes(x = x, y = y), fill = NA, colour = alpha(\"red\", 0.25), size = 0.5) +  \n    coord_sf(xlim = c(-118.96250491825595 - 0.1, -118.96250491825595 + 0.1),\n             ylim = c(34.05315253542454 - 0.1, 34.05315253542454 + 0.1)) + \n    theme_map()\n\n\n\n\n\n\n\n\nZooming in on Neptune’s Net (“Johnny, gimme two!”) demonstrates a little issue: these rasters aren’t perfectly aligned, but they’ll need to be in order to use the population raster as weights when we compute average weather for the county. (Actually, exactextractr could deal with it on its own, but it’s not very good practice.) How can we deal with this? It’s easy: we can resample the population raster to match the weather raster.\n\n\nTime to resample!\n\npop_rs = raster::resample(pop_crop, weather_crop)\npop_df2 = as.data.frame(pop_rs, xy = TRUE)\n\nggplot(LA_county) + \n    geom_sf() + \n    geom_tile(data = weather_df, aes(x = x, y = y), fill = NA, colour = \"black\") +  \n    geom_tile(data = pop_df2, aes(x = x, y = y), fill = NA, colour = alpha(\"red\", 0.25), size = 0.5) +  \n    coord_sf(xlim = c(-118.96250491825595 - 0.1, -118.96250491825595 + 0.1),\n             ylim = c(34.05315253542454 - 0.1, 34.05315253542454 + 0.1)) + \n    theme_map()\n\n\n\n\n\n\n\n\nNow that the rasters are aligned, we can move forward. “Little hand says it’s time to rock and roll.”\n\n\nComputing population-weighted averages\nThere’s not much left to do, now that the exact_extract command exists. We can compute both unweighted and weighted averages for LA county (they’re different by more than 1 degree C), and we can compute weighted averages for all the counties in California.\n\nexact_extract(weather_crop, LA_county, fun = \"weighted_mean\", weights = pop_rs)\n# Compare to unweighted mean\nexact_extract(weather_crop, LA_county, fun = \"mean\")\n\n# Now use uncropped (and re-sampled) rasters to generate pop-weighted averages for all CA counties\npop_rast_rs = raster::resample(pop_rast, weather_rast)\nCA_counties$tmean_averages = exact_extract(weather_rast, CA_counties, fun = \"weighted_mean\", weights = pop_rast_rs)\n\nggplot(CA_counties) + \n    geom_sf(aes(fill = tmean_averages)) + \n    scale_fill_viridis_c(name = NULL) + \n    theme_map() +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nAnd that’s it! Note that you can use exact_extract on other types of raster objects, like RasterStacks or RasterBricks, which allow you to compute averages on more than one layer at once. Happy population-weighting, or in the immortal words of Bodhi…\n\n\n\n“Vaya con dios, brah.”"
  },
  {
    "objectID": "posts/2019-11-25-r-reg-tables/2019-11-25-r-reg-tables.html",
    "href": "posts/2019-11-25-r-reg-tables/2019-11-25-r-reg-tables.html",
    "title": "Generating regression tables in R",
    "section": "",
    "text": "I often need to document the statistical results I estimate in table format. I have tried many, many things over the years, and none of my solutions are perfect, including the one I’m about to describe. But, it is now… pretty good.\nFirst, I define a function that takes a list of fitted models (models) and some other variables and outputs a list of pieces that I can create a table with. See below for the function definition.\n\nlibrary(stargazer)\n\n#' @param models A list of fitted models that stargazer can process\n#' @param keep Length 1 character vector of variables to display in table\n#' @param covariate.labels Labels for keep\n#' @param digits Number of digits to use for numbers in the table\n#'\n#' @return List of pieces of a tabular with named items header, inner, and footer\nmake_tex_pieces &lt;- function(models, keep, covariate.labels, digits = 2) {\n  # models: a \n  # Use stargazer, but keep as little extra stuff as possible\n  tex_raw &lt;- stargazer(models, \n                       keep = keep, covariate.labels = covariate.labels, \n                       digits = digits, \n                       table.layout = \"t\", no.space = T, align = T)\n  \n  # Split up into header, footer, and inner\n  idx0 &lt;- grep(\"begin{tabular}\", tex_raw, fixed = T) # Start of \\begin{tabular}\n  idx1 &lt;- grep(\"end{tabular}\", tex_raw, fixed = T) # End of \\begin{tabular}\n  \n  tex_header &lt;- c(tex_raw[idx0], \"\\\\toprule\")\n  tex_footer &lt;- c(\"\\\\bottomrule\", tex_raw[idx1])\n  \n  # Remove [-1.8ex] and get the inside of the tabular\n  tex_inner &lt;- gsub(\"\\\\\\\\[-[\\\\.0-9]+ex]\", \"\", tex_raw[(idx0+1):(idx1-1)])\n  \n  # Return these as a 3 element list so that the user can insert header rows (column labels)\n  # and footer rows (summary statistics, fixed effects)\n  list(header = tex_header, inner = tex_inner, footer = tex_footer)\n}\n\nOnce I have that function defined, I can use it to create the inside part of the table: the tabular command.\n# Load a sample dataset and run regression\ndata(cars)\nfit &lt;- lm(speed ~ dist, data = cars)\n\n# Use the function we defined above to split the regression output into different pieces of a tabulr\npieces &lt;- make_tex_pieces(fit, \"dist\", \"distance\")\n% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com % Date and time: Sat, Dec 10, 2022 - 23:36:33 % Requires LaTeX packages: dcolumn\n# Put the pieces back together, adding a short panel with the count of observations\nlatex_output &lt;- c(pieces$header,\n                  pieces$inner,\n                  \"\\\\midrule\",\n                  sprintf(\"Observations & %.0f \\\\\\\\\", length(fit$model$dist)),\n                  pieces$footer)\n\n# Write to file (I leave commented)\n# write(latex_output, \"model-tabular.tex\") \nlatex_output\n[1] “\\begin{tabular}{@\\\\extracolsep{5pt}lD{.}{.}{-2} }” [2] “\\toprule”\n[3] ” distance & 0.17^{***} \\\\ ”\n[4] ” & (0.02) \\\\ ”\n[5] “\\midrule”\n[6] “Observations & 50 \\\\”\n[7] “\\bottomrule”\n[8] ” \\end{tabular} ”\nNext, I use the LaTeX threeparttable package (also used in this post) to display the table. Here’s a minimum example.\n\\documentclass{article}\n\n\\usepackage{booktabs} % Nice-looking tables\n\\usepackage{dcolumn} % Booktabs column spacing\n\\usepackage{threeparttable} % Align column caption, table, and notes\n\n% Flexible notes environment based on minipage\n\\newenvironment{notes}[1][Notes]{\\begin{minipage}[t]{\\linewidth}\\normalsize{\\itshape#1: }}{\\end{minipage}}\n\n\\begin{document}\n\n\\begin{table}\n  \\centering\n  \\begin{threeparttable}\n  \\caption{My table}\n  \\input{model-tabular.tex}\n    \\begin{notes}\n    * p $&lt;$ 0.1, ** p $&lt;$ 0.05, *** p $&lt;$ 0.01. This regression is not confounded at all.\n    \\end{notes}\n  \\end{threeparttable}\n\\end{table}\n\n\\end{document}\nAnd here’s the result.\n\n\n\nTable output\n\n\nOther packages you might find useful:\n\nhuxtable is a good solution for generating quick regression tables for export to Markdown or HTML. I find its LaTeX output functions fairly cumbersome.\nkable/kableExtra are great for general purpose table creation, but can’t easily process fitted model output."
  },
  {
    "objectID": "posts/2022-05-13-envecon-datasets/2022-05-13-envecon-datasets.html",
    "href": "posts/2022-05-13-envecon-datasets/2022-05-13-envecon-datasets.html",
    "title": "A catalog of datasets for environmental economics",
    "section": "",
    "text": "With the help of Keith Kirchner, one of our all-star graduate students, I’ve started to put together a catalog of datasets relevant to environmental economics research. I teach several classes where research design is a main emphasis, and many students have found it useful to have a list of datasets that are out there as they begin thinking about potential research ideas.\nThis post (a work in progress) captures our initial efforts to pull together a list. The loose inclusion criteria are data that might be useful for applied environmental economics research. The imagined audience is students in one of my courses, but I’m hopeful that it will be useful for others as well. Feel free to send me a note with other datasets that are useful.\n\n\n\n\n\n\n\n\n\nOther catalogs\n\nJPAL-North America’s Catalog of Administrative Datasets"
  },
  {
    "objectID": "posts/2018-10-08-readr-shortcuts/2018-10-08-readr-shortcuts.html",
    "href": "posts/2018-10-08-readr-shortcuts/2018-10-08-readr-shortcuts.html",
    "title": "Things I forget: readr shortcuts",
    "section": "",
    "text": "readr is a the swiss-army knife of data ingestion: it’s my tool of choice for reading in text data in R, not least because I’m spending more time using the tidyverse these days. The readr documentation is a little lacking in that it’s actually kind of hard to track down the single-character shortcuts for various column types. Without further ado:\n\n\"-\" = col_skip()\n\"?\" = col_guess()\nc = col_character()\nD = col_date()\nd = col_double()\ni = col_integer()\nl = col_logical()\nn = col_number()\nT = col_datetime()\nt = col_time()\n\nSource: https://github.com/tidyverse/readr/blob/master/R/col_types.R"
  },
  {
    "objectID": "posts/2018-08-28-nominatim-install-notes/2018-08-28-nominatim-install-notes.html",
    "href": "posts/2018-08-28-nominatim-install-notes/2018-08-28-nominatim-install-notes.html",
    "title": "Nominatim for offline geocoding",
    "section": "",
    "text": "I needed to geocode around 20 million addresses. Normally I could just pay geocod.io to do this for me, but because of the contractual confidentiality requirements around these addresses, it had to be done offline. So I went hunting for an offline geocoder and decided to go with Nominatim, though a blog post I found after I had mostly completed the install suggested that there may be better options: according to this helpful blog, Nominatim is most accurate within the range of addresses it geocodes, but it doesn’t geocode many addresses."
  },
  {
    "objectID": "posts/2018-08-28-nominatim-install-notes/2018-08-28-nominatim-install-notes.html#modifications-to-installation-procedure",
    "href": "posts/2018-08-28-nominatim-install-notes/2018-08-28-nominatim-install-notes.html#modifications-to-installation-procedure",
    "title": "Nominatim for offline geocoding",
    "section": "Modifications to installation procedure",
    "text": "Modifications to installation procedure\nThe tutorial is quite good, but there are a few things that I had to figure out and/or do differently:\n\nIt wasn’t totally obvious when to switch over to nominatim user, but it seems clear that the import should be done while logged in to that account.\nAlso had to set a password with sudo passwd nominatim before I could log into the account to install\nThe Apache configuration instructions are a little premature, need to actually install Nominatim first\nI installed osmium to merge files (sudo apt-get install osmium-tool)\n\n`osmium merge file1.pbf file2.pbf file3.pbf…\n\nModified the code to set up the Apache webserver to point to the correct Nominatim directory\nSkipped special key phrases\nNote that TIGER setup script (and some other scripts) need to be run from within the build/ directory\nDid not install any update capability (I want a static db)\n\nAlso, a useful hint: If you screw up the database import or just want to redo it, you can drop the database using dropdb nominatim."
  },
  {
    "objectID": "posts/2018-08-28-nominatim-install-notes/2018-08-28-nominatim-install-notes.html#postgresql-configuration",
    "href": "posts/2018-08-28-nominatim-install-notes/2018-08-28-nominatim-install-notes.html#postgresql-configuration",
    "title": "Nominatim for offline geocoding",
    "section": "Postgresql configuration",
    "text": "Postgresql configuration\nMy server has 24 cores, 128GB memory, and a 500GB SSD. These are the settings in /etc/postgresql/10/main/postgresql.conf that I used:\n{bash, eval = FALSE} shared_buffers (4GB) maintenance_work_mem (16GB) work_mem (50MB) effective_cache_size (24GB) synchronous_commit = off checkpoint_timeout = 10min checkpoint_completion_target = 0.9 fsync (off) -- (for import only, then set to on) full_page_writes (off) -- (for import only, then set to on)"
  },
  {
    "objectID": "posts/2018-08-28-nominatim-install-notes/2018-08-28-nominatim-install-notes.html#search-hierarchy",
    "href": "posts/2018-08-28-nominatim-install-notes/2018-08-28-nominatim-install-notes.html#search-hierarchy",
    "title": "Nominatim for offline geocoding",
    "section": "Search hierarchy",
    "text": "Search hierarchy\nNote that Nominatim will return no result if it has an excess of information. For example, 729 Oneida Pl., Madison WI 53713 returns no result (bad ZIP), but 729 Oneida Pl., Madison WI works. Search hierarchy:\n\nNUMBER, ADDRESS, CITY, STATE, ZIP\nNUMBER, ADDRESS, ZIP\nNUMBER, ADDRESS, CITY, STATE\nZIP\nCITY, STATE\n\nNote also that if you feed it a bad number (e.g., 600 Oneida Pl., Madison WI 53711, which does not exist) it will pick the centroid of the street."
  },
  {
    "objectID": "posts/2018-08-28-nominatim-install-notes/2018-08-28-nominatim-install-notes.html#parallelization",
    "href": "posts/2018-08-28-nominatim-install-notes/2018-08-28-nominatim-install-notes.html#parallelization",
    "title": "Nominatim for offline geocoding",
    "section": "Parallelization",
    "text": "Parallelization\nParallelizing the code provides a HUGE boost to performance. I use the multiprocessing Python package, which makes it relatively straightforward. Since I’m already using pandas in the code, I tried to use swifter to simplify the code, but I wasn’t able to get it to work."
  },
  {
    "objectID": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html",
    "href": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html",
    "title": "Notes on sourdough",
    "section": "",
    "text": "And now for something completely different… sourdough! I first began baking as an escape from grad school ennui. Since then, and especially in last couple months, it’s been fun to share a few of the tips and tricks I’ve picked up along the way with friends who are just getting into baking.\nThe notes that follow document some of what I’ve learned. Of course, they’re a work in progress and I’m learning more every time I bake. But maybe you’ll find them useful. I’ve split this post up into two sections: TL;DR (“Too Long; Didn’t Read”) walks you through the essential steps to making sourdough. If you want more, More detail, please delivers with some extra flavor, so to speak."
  },
  {
    "objectID": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html#timing",
    "href": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html#timing",
    "title": "Notes on sourdough",
    "section": "Timing",
    "text": "Timing\nMaking bread timing work for you is one of the key aspects to ensuring that this becomes a sustainable habit.\n\nI’ve found that starting around 5pm lets me get to bed on time and allows me to bake either the next morning (for a less sour loaf) or the next afternoon (for a more sour loaf).\nAlternatively, if you want to have a sour loaf in the morning, I would consider starting the morning before and letting it set from noon until 8am the next day.\n\nOnce you get used to the pattern of the bread, you can play with the timing to get it to fit your schedule. For instance, you can accelerate the process by letting the dough spend more time outside of the refrigerator, and vice versa."
  },
  {
    "objectID": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html#starting-a-starter",
    "href": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html#starting-a-starter",
    "title": "Notes on sourdough",
    "section": "Starting a starter",
    "text": "Starting a starter\n\nTo start a starter, mix 50/50 whole wheat and white flour together. You’re going to use a bunch of this mixture, so you might as well put together a lot of it. Tartine suggests five pounds.\nMix a handful of flour blend with water with your hands in a clear bowl (plastic or glass). It should have the thickness of a thick batter – no lumps. Leave it for 2-3 days.\nIf you have bubbles, you’re ready for the next stage. If you don’t have bubbles, wait another day or two. It’s fine if there’s a crust – just remove it.\nNow you’re going to start feeding it. Discard most of the culture, and add the flour mix and water in about equal weights (50 grams each or so). You want a thick batter still. You’ll do this for a couple weeks or so. Initially the starter will be fairly stinky. That should get better and it should smell more sweet/ripe later on. You want to feed it every 24 hours or so.\nYou’ll know the starter is ready when the smell is more yeast-like and when it ferments predictably. This means that it should rise 6-8 hours after you feed it and be larger in size. Then, you’re ready to mix.\nWhole grain or rye flours are better for getting started. This is because they have more enzymes available for helping the yeast to break down the carbohydrates. As the starter becomes healthier I move toward more AP flour. More whole grain or rye in the mix leads to a richer sour flavor.\nThin (more watery) starters work faster. This is because the water helps the yeast and bacteria break down the flour more quickly. They tend to have slightly less sour flavor.\nThick (more floury) starters work more slowly and seem to demonstrate their rise better. They tend to create a slightly more sour flavor. I almost always prefer to have a thick starter.\nBoth bacteria and yeast create CO2. Bacteria is responsible for the lactic and acetic acids that give sourdough it’s distinctive flavor."
  },
  {
    "objectID": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html#mixing",
    "href": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html#mixing",
    "title": "Notes on sourdough",
    "section": "Mixing",
    "text": "Mixing\n\nYou usually want to use a starter that you fed 6-8 hours ago, where it’s still gassy and has doubled in size. To get this, discard most of it (down to 20-30 grams, or about a tablespoon) and add 100 or 200 grams each of flour and water (100 if you’re making one loaf, 200 if two). After about 10ish hours it should be ready to bake. It will smell sweet like an overripe fruit — this is what the Tartine guy calls a young leaven, before it has become vinegary. This should pass the “float test” — if you stick a bit in a bowl of water, it will float to the top.\nUsing your chosen recipe (see bottom for mine), put in the (warm) water first. Then add the starter. It should float. Finally, add the flour and anything else except the salt. Mix it all together (I like to use a long-handled spatula but your hands are fine).\nNext you’ll cover it for the autolyse period. The autolyse is the 30-60 minute period after mixing the starter, flour, and water but before including the salt. It lets the yeast get started. The salt slows down fermentation (by dehydrating the dough) but has important qualities in gluten formation. I make this easy on myself by putting the salt in at the beginning (with a little more water) but not mixing it together right away.\nAfter the autolyse is over, scrunch the dough through your hands to mix the salt all the way in. Now wait 30 minutes and begin the folding process."
  },
  {
    "objectID": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html#folding-and-bulk-fermentation-first-rise",
    "href": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html#folding-and-bulk-fermentation-first-rise",
    "title": "Notes on sourdough",
    "section": "Folding (and bulk fermentation / first rise)",
    "text": "Folding (and bulk fermentation / first rise)\n\nFolding is the equivalent of kneading for sourdough. Both accomplish the same goal: encouraging the development of a strong, flexible gluten structure in the dough. But kneading is more appropriate with artificial yeasts, since they ferment so aggressively. For natural yeasts, folding is gentler and still allows for proper structure formation.\nThere are people who think that folding is not necessary, or at least has minimal impacts on the dough. They may be right.\nGluten and starch are the main parts of flour.\nGluten, which is basically chains of proteins + water, forms when flour and water mix.\nFolding helps the dough develop strong, flexible gluten chains.\nRelaxing allows the tightest chains to break\nGluten (along with a little natural fat/lipids in the flour) capture CO2 from fermentation.\nMore gluten means higher volume. But too much gluten will taste gross.\nYou are done folding/kneading when the dough is strong but flexible. This takes a bit of practice to judge.\nThe dough tends to go from lumpy and shaggy to smooth and almost shiny. It should stretch but not rip.\nIt is possible to overmix: this can lead to a sudden, total breakdown of the gluten.\nIn the process I use, the dough is effectively conducting its first rise during this time. For other types of bread, the folding and first rise are separate steps.\nStretch and fold demonstration I\nStretch and fold demonstration II\nAfter folding, I let the dough finish rising for an hour at least, and maybe a few hours more, to get more fermentation going.\nThis video has a good description of the whole sourdough process, including how to know when to finish the first rise."
  },
  {
    "objectID": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html#shaping",
    "href": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html#shaping",
    "title": "Notes on sourdough",
    "section": "Shaping",
    "text": "Shaping\n\nShaping is the process of giving shape to the dough.\nIn the shaping process, I first fold the dough over itself (four corners technique), and then drag or push it along the table surface I’m using to help develop strength and tautness in the surface. This is far easier to see through video than it is to learn through text.\nLots of ways to shape\n\nShaping with a very wet dough\nShaping with a slightly less wet dough\nAnother shaping example\n\nGoals of shaping\n\nRemove excess gas.\nSpread gas throughout the dough.\nCreate an even loaf.\nCreate a strong, tight surface to capture more gas as the dough continues to rise and to encourage more lift in the dough during the oven spring (see Baking section).\n\nGuidelines for shaping\n\nTouch the dough as little as possible. When touching the dough, keep hands floured.\nUse only a little flour on the table. Important to maintain some friction between the dough and the table to encourage tautness in the surface.\n\nI shape twice, with a 30 minute rest (covered) in between."
  },
  {
    "objectID": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html#proofing",
    "href": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html#proofing",
    "title": "Notes on sourdough",
    "section": "Proofing",
    "text": "Proofing\n\nAfter shaping the dough a second time, I put it into a banneton that’s been amply floured with a mix of wheat and rice flour (these seem to be better at releasing the dough later).\nI usually put a linen cloth in the banneton before putting the dough in it. This isn’t required (and means that I don’t get the nice banneton lines that you’ll see on some breads), but it’s much more difficult for even the wettest doughs to stick to linen.\nI cover the dough, usually with another linen cloth, maybe a layer of saran wrap (to keep moisture in) and then a kitchen town, wrap a rubber band or two around it, and put it into the fridge.\nI let the dough proof in the fridge for at least 12 hours before baking. Sometimes, if I want an especially sour flavor, I let it proof for 18 or 24 hours. This overproofs the dough a bit and it doesn’t rise as well, but I slightly prefer the more-sour flavor.\nAlternatively, I’ve also experimented with moving shaping later in the process. In this case, I finish mixing and then put the dough (in its container) into the fridge for about 12 hours (usually overnight). I think pull it out, let it warm up a bit, and then shape. It goes back into the fridge again for about an hour while the oven pre-heats and then I bake. This seems to give me more tension in the dough as it bakes, and a better rise than shaping before it goes in the fridge. However, I haven’t been able to get quite the same sour taste. A work in progress!"
  },
  {
    "objectID": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html#baking",
    "href": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html#baking",
    "title": "Notes on sourdough",
    "section": "Baking",
    "text": "Baking\n\nI bake the dough in a cast iron dutch oven, using the smaller part of the dutch oven (what would normally be the lid) as the base. The larger part of the dutch oven (what would normally be the base) is the top. The purpose of this is to preserve moisture for the first part of the bake. There are other techniques one could use, such as including a pan of water for the first part of the bake. But this is the easiest for me.\nI preheat the oven to 500 with the dutch oven inside.\nI remove the dough from the fridge and score (slice) it with a razor blade to give the gas a path to escape. Otherwise the dough explodes in an unsightly way.\nTurning the banneton upside, I drop the dough gently into the smaller part of the dutch oven, just removed from the oven. I put the larger part of the dutch oven on top of it and put it into the oven, turning the temperature down to 450.\nI bake it for about 15 minutes covered, remove the cover, and bake it for another 15-20 minutes until there is a nice browning on the crust.\nFinally, I pull it from the oven and let it cool on a rack.\nI don’t generally cut into the bread until about an hour from when it’s left the oven."
  },
  {
    "objectID": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html#tartine-country-loaf-recipe-makes-2-loaves",
    "href": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html#tartine-country-loaf-recipe-makes-2-loaves",
    "title": "Notes on sourdough",
    "section": "Tartine Country Loaf Recipe (makes 2 loaves)",
    "text": "Tartine Country Loaf Recipe (makes 2 loaves)\n\n\n\nIngredient\nAmount\n\n\n\n\nWater\n700 + 50 grams (covering the salt)\n\n\nStarter\n200 grams\n\n\nWhite flour\n900 grams\n\n\nWhole wheat flour\n100 grams\n\n\nSalt\n20 grams\n\n\n\n\nYou can find the NYT version of this recipe here.\n\n\nModifications\n\nI currently make a whole wheat country loaf. This has 600 grams of whole wheat and 400 grams of white. It doesn’t rise quite as well but the flavor is better and it’s better for you.\nI usually only make one loaf at a time. So I halve everything above.\nI don’t do a preferment or a poolish (yet), but am planning to try it soon."
  },
  {
    "objectID": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html#my-favorite-bread-books",
    "href": "posts/2020-04-12-bread-notes/2020-04-12-bread-notes.html#my-favorite-bread-books",
    "title": "Notes on sourdough",
    "section": "My favorite bread books",
    "text": "My favorite bread books\n\nFlour, Water, Salt, Yeast, Ken Forkish\n\nForkish’s treatise covers all kinds of bread, both naturally leavened and non. If I had to recommend a single book, it would be this one for both its variety and accessibility.\n\nTartine Bread, by Chad Robertson\n\nI learned to bake sourdough from this book. Their starter method and their country loaf recipe are still my rough templates. The book is definitely verbose and, let’s be real, a bit pretentious, but there’s no arguing with the quality of their product (if you’re ever in San Francisco, stopping by the Tartine bakery or manufactory is a must). They’ve written at least two more books since\n\nbread science: the chemistry and craft of making bread, by Emily Buehler\n\nVery useful to get down to the basic chemistry of bread-baking. The later chapters are more generally accessible and provide useful guidance on almost all stages of the baking process. Not specifically focused on sourdough, though."
  },
  {
    "objectID": "posts/2018-05-20-climate-projection/2018-05-20-climate-projection.html",
    "href": "posts/2018-05-20-climate-projection/2018-05-20-climate-projection.html",
    "title": "Climate Projection Sandbox",
    "section": "",
    "text": "Many climate-society papers project the impacts of predicted climate change on the outcome of interest (guilty!). This post include code to conduct this kind of “climate projection exercise”. The idea is to combine a dose-response function \\(f(T)\\) (i.e., a damage function) with an estimate of the projected shift in the distribution of climate \\(\\Delta g(T)\\). Specifically, damages are: \\[ \\int^T f(t) \\Delta g(t) dt \\]\nHowever, we’ll be estimating this integral numerically, because \\(f(T)\\) is annoying to integrate analytically and \\(g(T)\\) will be projected numerically. More on that later. First, import some packages\npacman::p_load(data.table, tidyverse, splints, survey, lfe, scales)\nSimulate a mostly-but-not-entirely increasing sinusoidal dose-response function, where \\(y\\) is some outcome we care about that is affected (in a weird way) by temperature.\nset.seed(42)\nN &lt;- 1000 \ndt &lt;- data.table(T = runif(N, 0, 40))\ndt[, y := -10 + 0.5 * T + 8 * sin(T / 6) + rnorm(N)]\n\nggplot(data = dt, aes(x = T, y = y)) + geom_point() + theme_minimal()"
  },
  {
    "objectID": "posts/2018-05-20-climate-projection/2018-05-20-climate-projection.html#predict-from-felm-output-with-svycontrast",
    "href": "posts/2018-05-20-climate-projection/2018-05-20-climate-projection.html#predict-from-felm-output-with-svycontrast",
    "title": "Climate Projection Sandbox",
    "section": "Predict from felm output with svycontrast",
    "text": "Predict from felm output with svycontrast\nPredict over the sequence using survey::svycontrast. This works with both lm and felm.\n\nres.list &lt;- list()\nfor (i in 1:nrow(Xp)) {\n  this.x &lt;- s[i]\n  this.row &lt;- c(unlist(Xp[i, ])) # Sometimes, will need to fill in values for other covariates (0s or means?), e.g. ppt\n  temp.dt &lt;- as.data.table(svycontrast(fit, this.row))\n  temp.dt[, x:=this.x]\n  setnames(temp.dt, c(\"coef\", \"se\", \"x\"))\n  res.list[[as.character(this.x)]] &lt;- temp.dt\n}\nresult_survey &lt;- rbindlist(res.list)\n\nggplot(data = result_survey, aes(x = x, y = coef)) + \n  geom_point(data = dt, mapping = aes(x = T, y = y), colour = \"blue\", alpha = 0.5) +\n  geom_line() +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2018-05-20-climate-projection/2018-05-20-climate-projection.html#predict-from-lm-output-with-predict",
    "href": "posts/2018-05-20-climate-projection/2018-05-20-climate-projection.html#predict-from-lm-output-with-predict",
    "title": "Climate Projection Sandbox",
    "section": "Predict from lm output with predict",
    "text": "Predict from lm output with predict\nAlternative to usingy survey::svycontrast loop is to do the same using predict.lm, but there is no equivalent predict.felm.\n\n# Note the list notation here. Need to tell predict that Xp is replacing X o\nresult_predict &lt;- as.data.table(predict(lmfit, newdata = list(X = Xp), se.fit = T))\nresult_predict[, s := s]\nggplot(data = result_predict, aes(x = s, y = fit)) + \n  geom_point(data = dt, mapping = aes(x = T, y = y), colour = \"blue\", alpha = 0.5) +\n  geom_line() +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2019-11-05-fd-vs-fe/2019-11-05-fd-vs-fe.html",
    "href": "posts/2019-11-05-fd-vs-fe/2019-11-05-fd-vs-fe.html",
    "title": "FD vs FE and new tidyr commands",
    "section": "",
    "text": "For two periods, first differences is numerically equivalent to a fixed effect model. The following code reproduces that result, using the (new) pivot_longer and pivot_wider commands from tidyr.\nlibrary(tidyverse)\nlibrary(lfe)\nlibrary(broom)\n\n# Create some fake data\n\ndata &lt;- as_tibble(expand.grid(group = c(\"A\", \"B\"),\n                              period = 0:1))\n\ndata &lt;- data %&gt;% \n    mutate(x = rnorm(nrow(.)))  %&gt;% \n    mutate(y = 3*x + period * 1.5 + rnorm(nrow(.)))\n\n# Long differences ----\n\n# Reshape long-then-wide so that variable names are row IDs, take differences, reshape wide again to get differences\nld &lt;- data %&gt;% \n    pivot_longer(c(x, y),\n                 names_to = \"var\") %&gt;%\n    pivot_wider(id_cols = c(group, var),\n                names_from = period,\n                names_prefix = \"period\",\n                values_from = \"value\") %&gt;%\n    mutate(diff = period1 - period0) %&gt;%\n    pivot_wider(id_cols = c(group), \n                names_from = var,\n                names_prefix = \"diff_\",\n                values_from = diff)\n\n# LD\ntidy(lm(diff_y ~ diff_x - 1, data = ld))\n\n# FE\ntidy(lm(y ~ x + group, data = data))"
  },
  {
    "objectID": "posts/2016-05-18-tweets-mongo/2016-05-18-tweets-mongo.html",
    "href": "posts/2016-05-18-tweets-mongo/2016-05-18-tweets-mongo.html",
    "title": "How to save a stream of tweets",
    "section": "",
    "text": "Note\n\n\n\nUpdate (July 2020): I no longer use this process, since now I save the raw tweets as JSONs and convert to CSVs for processing later. This takes up more hard drive space, but preserves the raw data.\n\n\nThe Twitter Streaming API returns each tweet as a JSON. My current implementation transforms that multi-level JSON object into a flat set of key-value pairs and saves those to a SQLite database. That transformation does not preserve the considerable amount of data available in the JSON, however.\nToday, I wanted to determine better strategy for saving the entire JSON. Major considerations are, in no particular order, storage space required, ability to consume throughput, and the time required to extra the raw data into a format ready for analysis. My working assumption from is that client will be consuming roughly 2.4 million tweets per day, though in reality it is probably more like 1.5. Three options on the table, including the current system:\n\nCurrent system: Save to relational DB on download\n\nPros\n\n0.3 gb per day (roughly)\nFairly fast to move to analysis\nCould be better for real-time\n\nCons\n\nLossy, we may want data for other projects\nKludgy code, data transformations make it more breakable\n\n\nJSON files: Save raw JSON, compress all tweets at the end of the day\n\nPros\n\nSaves all data\nNo need to deal with MongoDB\nWhen updating dataset, can just import dates not already included\nCleaner code\n\nCons\n\n1 gb per day (using daily bz2 compression)\n2.4m files created per day (file system stress)\nCompression is time-consuming (file system stress)\nPreparation for analysis could be time-consuming\n(Long-term) Difficult to use for real-time system?\n\n\nMongoDB: Save JSON to MongoDB\n\nPros\n\nSaves all data\nLittle stress on file system\nTransformation to data analysis should be easier than with files\nElegant, cleaner code\n\nCons\n\n1 gb per day (using zlib)\nzlib compression is comparable to bz2; code below db.create_collection('test', storageEngine={'wiredTiger':{'configString':'block_compressor=zlib'}})\nLack of knowledge about using MongoDB: unknown unknowns?\n\n\n\nUntil I have a server with a sufficient amount of space, I’m inclined to let the linux box continue to do its thing. Once I have access to a server with 1TB+ space, I like the MongoDB option."
  },
  {
    "objectID": "posts/2019-05-16-a-list-of-gists/2019-05-16-a-list-of-gists.html",
    "href": "posts/2019-05-16-a-list-of-gists/2019-05-16-a-list-of-gists.html",
    "title": "A list of gists",
    "section": "",
    "text": "I’ve been saving a lot of code snippets here. To keep things organized, I’m going to keep them in a single page instead.\nAn ideal solution would be to have an embedded, dynamically updating page that listed all of my gists by category or language. It’s not obvious to me how I can do that easily right now, although there is a Gist API.\nFor now, here is the list of all of my gists. And below is a hand-curated list by category:\n\nR\n\nUpdate beamer packages using TinyTex\nSpatial interpolation\nPadding integers with zeroes\nApply multiple functions to multiple columns (data.table)\nPoints in polys (sf)\ndplyr dynamic variable names\n\nPython\n\nUnclaimed property scraping code\n\nStata\n\nCompute relative humidity\nCompute heat index\nCreate nice dummy bins\n\nOther\n\nUsing tmux for remote commands\nOpen Bear with a daily log template"
  },
  {
    "objectID": "posts/2018-12-19-git-lfs/2018-12-19-git-lfs.html",
    "href": "posts/2018-12-19-git-lfs/2018-12-19-git-lfs.html",
    "title": "Things I forget: install git lfs and initialize in repo",
    "section": "",
    "text": "git lfs is great for including (fairly) large files in git repositories. Since the entire history of files is saved, it prevents large files from blowing up the repo. I’m not sure why it isn’t installed by default with git. Anyway, I always forget how to use it.\n\nInstalling git lfs\nEach computer accessing a repo with lfs enabled needs to have it installed, otherwise expected files may not appear and the user could (potentially) screw the repo up by deleting the wrong things. More instructions here, but for OS X I just run:\nbrew install git-lfs\ngit lfs install\nNote that this only needs to be run once per computer, not per repo.\n\n\nEnabling git lfs on a repo and adding files to it\nFor each repo, I need to tell git lfs which files or kinds of files to track. Suppose I want to use lfs for any csv files:\ngit lfs track '*.csv' '*.fst' '*.Rds' '*.gz' \"*.zip\"\ngit add .gitattributes '*.csv' '*.fst' '*.Rds' '*.gz' \"*.zip\"\nFirst line tells lfs which patterns to track, second adds .gitattributes and all of the relevant files (if necessary) to the commit.\n\n\nChecking in on git lfs\ngit lfs ls-files: Show a list of tracked files\n\n\nCleaning up an LFS repo\nI’m frequently finding myself at the limits of my GitHub LFS storage space. Here’s how I use BFG to do that. The following code clones a bare repo, drops anything bigger than 10m in the history, and pushes the update to GitHub.\n{bash, eval = FALSE} git clone --mirror git://example.com/some-big-repo.git java -jar bfg.jar --strip-blobs-bigger-than 10M some-big-repo.git cd some-big-repo.git git reflog expire --expire=now --all && git gc --prune=now --aggressive git push\nTo run this, you need to know which repo is eating up your LFS storage. That’s actually not straightforward, since (as far as I can tell) there’s no quick way to see the storage for all of your repos.\n\n\nSources\n\nInstall LFS\nLFS tutorial"
  },
  {
    "objectID": "posts/2016-03-17-zonal-analysis-speed/2016-03-17-zonal-analysis-speed.html",
    "href": "posts/2016-03-17-zonal-analysis-speed/2016-03-17-zonal-analysis-speed.html",
    "title": "Fast Zonal Statistics",
    "section": "",
    "text": "Warning\n\n\n\nUpdate (July 2020): There are now other R options that are faster. For example exactextractr, velox, and fasterize, among many others. I no longer resort to Python for these tasks.\n\n\nTaking the average value of rasters by polygon is slow in R. Really slow. Using extract with a raster of more than 800k cells (PRISM weather data) and a shapefile of more than 3000 polygons (counties) takes a little over 500 seconds.\nThe rasterstats package in Python offers a faster solution, but can be tricky to install, particularly in Windows:\n\nInstall Anaconda Python (trying to upgrade pip broke it, so I just reinstalled Ananconda)\nInstall gdal 1.11.4 (not 2.0), fiona, rasterio, and Shapely\n\nMay need to install some from binaries if using Windows\n\nInstall rasterstats\n\nSample code to run zonal statistics and save the result to CSV.\nimport csv\nfrom rasterstats import zonal_stats\n\n# GLOBALS\nRAST = 'E:/prism/data/zipped/PRISM_tmin_stable_4kmD1_20150531_bil.bil'\nPOLY = 'E:/prism/data/census/gz_2010_us_050_00_20m/gz_2010_us_050_00_20m.shp'\n\n# Run zonal_stats\nstats = zonal_stats(POLY, RAST, stats=['count', 'mean', 'min', 'max'], all_touched=True, geojson_out=True)\n\n# Save to CSV\nkeys = stats[0]['properties'].keys()\nwith open('E:/prism/out.csv', 'wb') as output_file:\n    dict_writer = csv.DictWriter(output_file, keys)\n    dict_writer.writeheader()\n    for row in stats:\n        dict_writer.writerow(dict((k, v.encode('utf-8')) if isinstance(v, basestring) else (k,v) for k, v in row['properties'].iteritems()))"
  },
  {
    "objectID": "posts/2018-09-11-tmux/2018-09-11-tmux.html",
    "href": "posts/2018-09-11-tmux/2018-09-11-tmux.html",
    "title": "Things I forget: tmux for headless SSH use",
    "section": "",
    "text": "How to run something through SSH that takes a long time without having to stay logged in:\n\nLog into ssh\nExecute tmux\nType command\nCTRL-b d to detach tmux. You can exit ssh.\nWhen returning to server, execute tmux attach\n\nReference"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "I’m an assistant professor and environmental economist at the Vancouver School of Economics at the University of British Columbia (UBC). I study how people respond to environmental threats like wildfires, air pollution, and extreme temperatures.\nSome of my recent work includes evaluating the effectiveness of wildland building homes on home survival in a wildfire, assessing the implicit subsidy of federal wildfire suppression, estimating the impact of temperature on tweets to understand preferences for climate change, and considering how consumers respond to defaults in electricity consumption.\nMany of my projects use large datasets, natural language processing, spatial information, or all three; as a result, I do most of my data work in R/RStudio and Python. When not in front of my computer, you might find me running, baking sourdough, or throwing a frisbee.\nBefore I joined UBC, I was a postdoctoral fellow at the Stanford Center on Food Security and the Environment, a graduate student in Agricultural and Resource Economics at UC-Berkeley, and a research assistant at the Energy Institute at Haas."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Making LaTeX tables that look nice (2024 update)\n\n\n\n\n\n\nR\n\n\nlatex\n\n\n\n\n\n\n\n\n\nApr 6, 2024\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Quarto\n\n\n\n\n\n\nwriting\n\n\ncoding\n\n\n\nIt’s Quarto’s world, we’re all just living in it\n\n\n\n\n\nDec 11, 2022\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nA catalog of datasets for environmental economics\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\nMay 13, 2022\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nProductivity and the work habits that work (for me)\n\n\n\n\n\n\nproductivity\n\n\n\n\n\n\n\n\n\nFeb 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPop-weighted averages from rasters\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nAug 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nMaking better tables\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nPartial predictions\n\n\n\n\n\n\ncoding\n\n\n\n\n\n\n\n\n\nJan 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHow I’m remote teaching a big class this fall\n\n\n\n\n\n\nteaching\n\n\n\n\n\n\n\n\n\nAug 20, 2020\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nOverlaying a raster and shapefile\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJul 3, 2020\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on sourdough\n\n\n\n\n\n\nfood\n\n\n\n\n\n\n\n\n\nApr 12, 2020\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nHow to plot a specification curve\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nFeb 28, 2020\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nScoring texts for the presence of phrases\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nDec 5, 2019\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating regression tables in R\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nFD vs FE and new tidyr commands\n\n\n\n\n\n\neconometrics\n\n\n\n\n\n\n\n\n\nNov 5, 2019\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nMaking regressions purrr\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nA list of gists\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nMay 16, 2019\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nThings I forget: install git lfs and initialize in repo\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nNov 4, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatically size beamer images and tables\n\n\n\n\n\n\nwriting\n\n\n\n\n\n\n\n\n\nOct 11, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nThings I forget: readr shortcuts\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nOct 8, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nDebugging in R, RStudio\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nSep 11, 2018\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nThings I forget: tmux for headless SSH use\n\n\n\n\n\n\nproductivity\n\n\n\n\n\n\n\n\n\nSep 11, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nNominatim for offline geocoding\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nClimate Projection Sandbox\n\n\n\n\n\n\neconometrics\n\n\n\n\n\n\n\n\n\nMay 20, 2018\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nLinear combinations of coefficients in R\n\n\n\n\n\n\ncoding\n\n\n\n\n\n\n\n\n\nApr 12, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nR performance tests\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nFeb 3, 2017\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating high dimensional fixed effects models in Julia\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nJan 19, 2017\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nYesterday’s Maximum Temperature is… Today’s Maximum Temperature?\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\nMay 18, 2016\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nHow to save a stream of tweets\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\nMay 18, 2016\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nFast Zonal Statistics\n\n\n\n\n\n\neconometrics\n\n\n\n\n\n\n\n\n\nMar 17, 2016\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nConley standard errors and high dimensional fixed effects\n\n\n\n\n\n\neconometrics\n\n\n\n\n\n\n\n\n\nOct 12, 2015\n\n\nPatrick Baylis\n\n\n\n\n\n\n\n\n\n\n\n\nProductivity and organization notes\n\n\n\n\n\n\nproductivity\n\n\n\n\n\n\n\n\n\nSep 8, 2015\n\n\nPatrick Baylis\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Other\n\nhttps://datastudio.google.com/u/0/\n\n\n\nData\n(Consolidate with blog post where I list data) - Pyregence (Open source fire forecasts) data sources: https://pyregence.org/wildfire-forecasting/data-repository/\n\n\nR Packages\n\nROpenSci Universe: The ROpenSci list of packages includes many that facilitate access to interesting datasets.\nProgramming\n\nskimr: Quick summary statistics.\ncharlatan: Make fake data, e.g., names.\nbibtex: Parse a .bib file. (Might be useful for this website. There are other related packages, e.g., handlr.)\nparzer: Parse messy latitudes and longitudes.\n\nWeather\n\nweathercan: R package to get ECCC weather data.\nGSODR: One of a few packages for downloading Global Summary of the Day weather data.\nrnoaa: Client for many NOAA data sources.\nstationaRy: Hourly weather data from around the world.\nprism: Access prism data.\nChirps and Chirts: Global reanalysis weather data, 1983-2016.\n\nDisasters\n\nrrricanes: Hurricane data since 1998.\n\nPollution\n\nepair: Help with parsing various EPA datasets.\n\nImagery\n\nrsat: Search, download, and process satellite imagery from several free sources.\nrnaturalearth: Natural earth imagery for mapping.\n\nTransport\n\nbikedata: Public bicycle data.\n\nSpatial\n\ngeonames: Access geonames.org API to get geography names."
  },
  {
    "objectID": "posts/2022-12-11-welcome-to-quarto/welcome-to-quarto.html",
    "href": "posts/2022-12-11-welcome-to-quarto/welcome-to-quarto.html",
    "title": "Welcome to Quarto",
    "section": "",
    "text": "About six months ago, the team at RStudio (now Posit) released Quarto v1.0, a new “open-source scientific and technical publishing system.” At the time, it barely crossed my radar: it looked and sounded a lot like a repackaging of RMarkdown, so I quickly scanned the press release and immediately forgot about it. I’m already a heavy Markdown user – whenever possible I prefer to write in plain text, or close to it, rather than in something with more complicated syntax like raw HTML or, god forbid, LaTeX. I thought that the tools I was using were good enough, and didn’t feel the need to switch.\nIt’s only in the last few weeks that I’ve come to appreciate how much better Quarto is than everything it replaces, at least from what I’ve seen so far.1 So in the hopes of helping those of you who have ignored Quarto like I did, here are three specific ways that switching to Quarto has made my life easier:"
  },
  {
    "objectID": "posts/2024-04-06-tables-update/tables-update.html",
    "href": "posts/2024-04-06-tables-update/tables-update.html",
    "title": "Making LaTeX tables that look nice (2024 update)",
    "section": "",
    "text": "I haven’t been very good at updating the blog recently,1 but it feels like as good a time as any for an update on how to make basic but nice-looking LaTeX tables with relatively little coding effort. This post is an update to this post from 2021, which was itself an update to a post from 2019. I’ll leave the discussion of philosophies around table-making to the 2021 post and get right to the good stuff: we’re going to make these two tables.\nIf you don’t like the way those look, now is a good time to bail."
  },
  {
    "objectID": "posts/2024-04-06-tables-update/tables-update.html#footnotes",
    "href": "posts/2024-04-06-tables-update/tables-update.html#footnotes",
    "title": "Making LaTeX tables that look nice (2024 update)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn understatement. I’ve been very bad at updating it.↩︎"
  },
  {
    "objectID": "research.html#working-papers",
    "href": "research.html#working-papers",
    "title": "Research",
    "section": "Working papers",
    "text": "Working papers\n\nIs the Demand for Clean Air Too Low? Experimental Evidence from Delhi (with Michael Greenstone, Kenneth Lee, and Harshil Sahai)\n\nWorking Paper\n\nIncentivizing Self-Protection from Wildfires (with Judson Boomhower and Bob Horton)\n\nWorking Paper \\(\\cdot\\) J-PAL blog post\n\nDoes Time Shift Behavior? The Clock- vs. Solar-time Tradeoff (with Severin Borenstein and Ed Rubin)\n\nWorking Paper \\(\\cdot\\) NBER Working Paper (March 2023) \\(\\cdot\\) Blog post\n\nClimate and Migration in the United States (with Prashant Bharadwaj, Jamie Mullins, and Nick Obradovich)\n\nWorking Paper"
  },
  {
    "objectID": "posts/2021-05-30-making-tables/2021-05-30-making-tables.html#footnotes",
    "href": "posts/2021-05-30-making-tables/2021-05-30-making-tables.html#footnotes",
    "title": "Making better tables",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlternatively, if you prefer not to rescale you may need to format each one as a string instead. This takes a bit more work.↩︎"
  },
  {
    "objectID": "posts/2021-01-22-predict-partial/2021-01-22-predict-partial.html#footnotes",
    "href": "posts/2021-01-22-predict-partial/2021-01-22-predict-partial.html#footnotes",
    "title": "Partial predictions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’m not actually sure we need to assume this, but someone smarter than me can work out the math of what we could do in the non-linear-in-parameters case.↩︎\nThis code borrows from code written by Kenny Bell in this StackOverflow post.↩︎"
  },
  {
    "objectID": "posts/2022-12-11-welcome-to-quarto/welcome-to-quarto.html#footnotes",
    "href": "posts/2022-12-11-welcome-to-quarto/welcome-to-quarto.html#footnotes",
    "title": "Welcome to Quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd, it’s being updated very rapidly. As I write this on a Sunday, I see the pre-release notes for v1.3 were also released today.↩︎"
  },
  {
    "objectID": "posts/2024-04-06-tables-update/tables-update.html#the-final-product",
    "href": "posts/2024-04-06-tables-update/tables-update.html#the-final-product",
    "title": "Making LaTeX tables that look nice (2024 update)",
    "section": "The final product",
    "text": "The final product\nJust so you know where this is going, we’re going to make these two tables."
  },
  {
    "objectID": "posts/2024-04-06-tables-update/tables-update.html#setup",
    "href": "posts/2024-04-06-tables-update/tables-update.html#setup",
    "title": "Making LaTeX tables that look nice (2024 update)",
    "section": "Setup",
    "text": "Setup\n\nLoad stuff\nFirst, we gotta load some stuff. I’m going to use pacman for package management, the tidyverse for general data manipulation, AER for some data on automobile fatatalies, modelsummary for summary statistics, kableExtra for some extra table formatting, and the inimitable fixest for estimating regressions and making the regression tables.\n\n\n\n\n\n\nNote\n\n\n\nWe owe these package authors a huge thank-you for making our lives easier. If you use them, don’t forget to cite them in your papers and buy them a beverage of their choice when you meet them in person!\n(Mea culpa: I have not done a good job citing the many packages I use in my own work. I will do better.)\n\n\n\npacman::p_load(tidyverse, AER, modelsummary, kableExtra, fixest)\n\ndata(\"Fatalities\")\n\n\n\nRescale and relabel variables\nNext we need to do some cleaning up. I find it’s useful to have my variables on similar scales for regressions and I want to make sure I’m describing those variables accurately, so before I do anythign I rescale and rename variable appropriately. This is also a good time to define a data dictionary here to clean up names in tables later.\n\n## Rescale and label variables\ndata &lt;- Fatalities %&gt;%\n  transmute(state, year, \n    unemprate = unemp,\n    beertax = beertax * 100,\n    income1000s = income / 1000,\n    miles1000s = miles / 1000,\n    fatal1000s = fatal / 1000,\n    popm = pop / 1e6\n  )\n\nvar_dict &lt;- c(\n  unemprate = \"Unemployment rate\",\n  beertax = \"Beer tax\",\n  income1000s = \"Income (1000s)\",\n  miles1000s = \"Miles (1000s)\",\n  fatal1000s = \"Fatalities (1000s)\",\n  popm = \"Population (millions)\"\n)"
  },
  {
    "objectID": "posts/2024-04-06-tables-update/tables-update.html#descriptive-statistics",
    "href": "posts/2024-04-06-tables-update/tables-update.html#descriptive-statistics",
    "title": "Making LaTeX tables that look nice (2024 update)",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\nWe’re going to make a two-panel descriptive statistics table. The first panel will summarize the variable distributions, and the second will count the total and unique observations by group.\n\nSummarize variable distributions\nNow we can summarize the variable distributions, the standard approach for a summary statistics table in a paper. modelsummary::datasummary does most of the legwork here for us.\n\nvariable_distributions &lt;- datasummary(All(data) ~ Mean + SD + Min + Median + Max,\n  fmt = fmt_significant(2),\n  output = \"data.frame\",\n  data = data\n)\n\n# Use the data dictionary to rename variables\nvariable_distributions &lt;- variable_distributions %&gt;% \n  mutate(` ` = var_dict[as.character(` `)])\n\nvariable_distributions\n\n                        Mean   SD   Min Median Max\n1     Unemployment rate  7.3  2.5   2.4      7  18\n2              Beer tax   51   48   4.3     35 272\n3        Income (1000s)   14  2.3   9.5     14  22\n4         Miles (1000s)  7.9  1.5   4.6    7.8  26\n5    Fatalities (1000s) 0.93 0.93 0.079    0.7 5.5\n6 Population (millions)  4.9  5.1  0.48    3.3  28\n\n\n\n\nCount total and unique observations by group\nI also like to count the total observations and unique observations by group (often different sets of fixed effects) in the same table.\n\n# Count total observations and unique observations by group\ncounts &lt;- tribble(\n  ~Name, ~Value,\n  \"Observations\", sprintf(\"%g\", nrow(data)),\n  \"States\", sprintf(\"%g\", n_distinct(data$state)),\n  \"Years\", sprintf(\"%g\", n_distinct(data$year))\n)\ncounts\n\n# A tibble: 3 × 2\n  Name         Value\n  &lt;chr&gt;        &lt;chr&gt;\n1 Observations 336  \n2 States       48   \n3 Years        7    \n\n\n\n\nTurn into LaTeX\nNow we can combine the two tables and save them as .tex files. This requires a tiny bit of hacking to jam the tables together. Note that save the output using the output = \"latex_tabular\" option to get only the tabular environment. This is useful because it lets me set captions and tables notes in the LaTeX document itself.\n\n# To combine, align first names\nnames(counts)[1:ncol(counts)] &lt;- names(variable_distributions)[1:ncol(counts)]\n\nsummary_stats &lt;- bind_rows(variable_distributions, counts)\n\n# Replace NAs with empty strings\nsummary_stats[is.na(summary_stats)] &lt;- \" \"\n\ndatasummary_df(\n  summary_stats,\n  output = \"latex_tabular\"\n) %&gt;%\n  group_rows(\"Variables\", start_row = 1, end_row = nrow(variable_distributions), bold = F, italic = T) %&gt;%\n  group_rows(\"Counts\", start_row = nrow(variable_distributions) + 1, end_row = nrow(summary_stats), bold = F, italic = T) %&gt;%\n  write_lines(\"summary-stats.tex\")"
  },
  {
    "objectID": "posts/2024-04-06-tables-update/tables-update.html#regression-table",
    "href": "posts/2024-04-06-tables-update/tables-update.html#regression-table",
    "title": "Making LaTeX tables that look nice (2024 update)",
    "section": "Regression table",
    "text": "Regression table"
  },
  {
    "objectID": "posts/2024-04-06-tables-update/tables-update.html#regression-estimates",
    "href": "posts/2024-04-06-tables-update/tables-update.html#regression-estimates",
    "title": "Making LaTeX tables that look nice (2024 update)",
    "section": "Regression estimates",
    "text": "Regression estimates\nI’m a big fan of fixest for estimating regressions. It is extremely fast for models with lots of observations and fixed effects, and it includes a very nice set of functions for summarizing regression output. First we’ll estimate the models, then we’ll make the table.\n\nEstimate regression models\nMy tables often use similar sets of control variables with different sets of fixed effects. Content note: this is a regression annual automobile fatalities on a few variables, including the unemployment rate, income, beer taxes, and miles driven. The causal inference economist in me is required to disclose that I don’t think we’re estimating true causal effects here, but it’s a nice example for a table. I use use etable at the end of this chunk for a quick view of the estimates.\n\nfmla &lt;- \"fatal1000s ~ unemprate + income1000s + beertax + miles1000s\"\n\nmodels &lt;- list()\nmodels[[\"OLS\"]] &lt;- feols(as.formula(fmla), data = data)\nmodels[[\"+ State FE\"]] &lt;- feols(as.formula(sprintf(\"%s | state\", fmla)), data = data)\nmodels[[\"+ Year FE\"]] &lt;- feols(as.formula(sprintf(\"%s | state + year\", fmla)), data = data)\n\netable(models, cluster = ~state)\n\n                              OLS         + State FE          + Year FE\nDependent Var.:        fatal1000s         fatal1000s         fatal1000s\n                                                                       \nConstant          -3.401* (1.649)                                      \nunemprate       0.1371** (0.0483) -0.0179** (0.0066) -0.0322** (0.0105)\nincome1000s      0.2250* (0.0870)    0.0200 (0.0140)   0.0365. (0.0186)\nbeertax         0.0048** (0.0016)  -0.0025* (0.0012)  -0.0028. (0.0015)\nmiles1000s       -0.0060 (0.0507)   -0.0021 (0.0037)    0.0016 (0.0032)\nFixed-Effects:  ----------------- ------------------ ------------------\nstate                          No                Yes                Yes\nyear                           No                 No                Yes\n_______________ _________________ __________________ __________________\nS.E.: Clustered         by: state          by: state          by: state\nObservations                  336                336                336\nR2                        0.17631            0.99245            0.99326\nWithin R2                      --            0.26298            0.25553\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nTurn into LaTeX\nBut etable is capable of much more. Below I set options appropriate for LaTeX output.\n\nsetFixest_dict(var_dict)\n\netable(models,\n  cluster = ~state,\n  drop = \"Constant\",\n  digits = \"r3\",\n  digits.stats = 2,\n  fitstat = c(\"n\", \"r2\"),\n  style.tex = style.tex(\"aer\",\n    fixef.suffix = \" FEs\",\n    fixef.where = \"var\",\n    yesNo = c(\"Yes\", \"No\")\n  ),\n  tex = T\n) %&gt;%\n  write_lines(\"regression-etable.tex\")"
  },
  {
    "objectID": "posts/2024-04-06-tables-update/tables-update.html#latex-document",
    "href": "posts/2024-04-06-tables-update/tables-update.html#latex-document",
    "title": "Making LaTeX tables that look nice (2024 update)",
    "section": "LaTeX document",
    "text": "LaTeX document\nThat’s basically it. The last step is to plonk these into a LaTeX document (the output of which you saw above), copied below. It’s fairly simple, but I do use the threeparttable package to add notes to the tables.\n\\documentclass{article}\n\\usepackage{booktabs} \n\\usepackage{caption}\n\\usepackage[para]{threeparttable}\n\\usepackage{siunitx}\n\n\\begin{document}\n\n\\begin{table}\n\\centering\n\\begin{threeparttable}\n\\caption{Summary statistics}\n\\input{summary-stats}\n\\begin{tablenotes}\n \\item \\emph{Notes}: Table notes here.\n\\end{tablenotes}\n\\end{threeparttable}\n\\end{table}\n\n\\begin{table}\n\\centering\n\\begin{threeparttable}\n\\caption{Regression estimates}\n\\input{regression-etable}\n\\begin{tablenotes}\n \\item \\emph{Notes}: Table notes here.\n\\end{tablenotes}\n\\end{threeparttable}\n\\end{table}\n\n\\end{document}"
  },
  {
    "objectID": "posts/2024-04-06-tables-update/index.html",
    "href": "posts/2024-04-06-tables-update/index.html",
    "title": "Making LaTeX tables that look nice (2024 update)",
    "section": "",
    "text": "I haven’t been very good at updating the blog recently,1 but it feels like as good a time as any for an update on how to make basic but nice-looking LaTeX tables with relatively little coding effort. This post is an update to this post from 2021, which was itself an update to a post from 2019. I’ll leave the discussion of philosophies around table-making to the 2021 post and get right to the good stuff: we’re going to make these two tables.\nIf you don’t like the way those look, now is a good time to bail."
  },
  {
    "objectID": "posts/2024-04-06-tables-update/index.html#setup",
    "href": "posts/2024-04-06-tables-update/index.html#setup",
    "title": "Making LaTeX tables that look nice (2024 update)",
    "section": "Setup",
    "text": "Setup\n\nLoad stuff\nFirst, we gotta load some stuff. I’m going to use pacman for package management, the tidyverse for general data manipulation, AER for some data on automobile fatatalies, modelsummary for summary statistics, kableExtra for some extra table formatting, and the inimitable fixest for estimating regressions and making the regression tables.\n\n\n\n\n\n\nNote\n\n\n\nWe owe these package authors a huge thank-you for making our lives easier. If you use them, don’t forget to cite them in your papers and buy them a beverage of their choice when you meet them in person!\n(Mea culpa: I have not done a good job citing the many packages I use in my own work. I will do better.)\n\n\n\npacman::p_load(tidyverse, AER, modelsummary, kableExtra, fixest)\n\ndata(\"Fatalities\")\n\n\n\nRescale and relabel variables\nNext we need to do some cleaning up. I find it’s useful to have my variables on similar scales for regressions and I want to make sure I’m describing those variables accurately, so before I do anything I rescale and rename variables appropriately. This is also a good time to define a data dictionary here to clean up displayed variable names later.\n\n## Rescale and label variables\ndata &lt;- Fatalities %&gt;%\n  transmute(state, year, \n    unemprate = unemp,\n    beertax = beertax * 100,\n    income1000s = income / 1000,\n    miles1000s = miles / 1000,\n    fatal1000s = fatal / 1000,\n    popm = pop / 1e6\n  )\n\nvar_dict &lt;- c(\n  unemprate = \"Unemployment rate\",\n  beertax = \"Beer tax\",\n  income1000s = \"Income (1000s)\",\n  miles1000s = \"Miles (1000s)\",\n  fatal1000s = \"Fatalities (1000s)\",\n  popm = \"Population (millions)\"\n)"
  },
  {
    "objectID": "posts/2024-04-06-tables-update/index.html#descriptive-statistics",
    "href": "posts/2024-04-06-tables-update/index.html#descriptive-statistics",
    "title": "Making LaTeX tables that look nice (2024 update)",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\nWe’re going to make a two-panel descriptive statistics table. The first panel will summarize the variable distributions, and the second will count the total and unique observations by group.\n\nSummarize variable distributions\nNow we can summarize the variable distributions, the standard approach for a summary statistics table in a paper. modelsummary::datasummary does most of the legwork here for us.\n\nvariable_distributions &lt;- datasummary(All(data) ~ Mean + SD + Min + Median + Max,\n  fmt = fmt_significant(2),\n  output = \"data.frame\",\n  data = data\n)\n\n# Use the data dictionary to rename variables\nvariable_distributions &lt;- variable_distributions %&gt;% \n  mutate(` ` = var_dict[as.character(` `)])\n\nvariable_distributions\n\n                        Mean   SD   Min Median Max\n1     Unemployment rate  7.3  2.5   2.4      7  18\n2              Beer tax   51   48   4.3     35 272\n3        Income (1000s)   14  2.3   9.5     14  22\n4         Miles (1000s)  7.9  1.5   4.6    7.8  26\n5    Fatalities (1000s) 0.93 0.93 0.079    0.7 5.5\n6 Population (millions)  4.9  5.1  0.48    3.3  28\n\n\n\n\nCount total and unique observations by group\nI also like to count the total observations and unique observations by group (often different sets of fixed effects) in the same table.\n\n# Count total observations and unique observations by group\ncounts &lt;- tribble(\n  ~Name, ~Value,\n  \"Observations\", sprintf(\"%g\", nrow(data)),\n  \"States\", sprintf(\"%g\", n_distinct(data$state)),\n  \"Years\", sprintf(\"%g\", n_distinct(data$year))\n)\ncounts\n\n# A tibble: 3 × 2\n  Name         Value\n  &lt;chr&gt;        &lt;chr&gt;\n1 Observations 336  \n2 States       48   \n3 Years        7    \n\n\n\n\nTurn into LaTeX\nNow we can combine the two tables and save them as .tex files. This requires a tiny bit of hacking to jam the tables together. Note that save the output using the output = \"latex_tabular\" option to get only the tabular environment. This is useful because it lets me set captions and tables notes in the LaTeX document itself.\n\n# To combine, align first names\nnames(counts)[1:ncol(counts)] &lt;- names(variable_distributions)[1:ncol(counts)]\n\nsummary_stats &lt;- bind_rows(variable_distributions, counts)\n\n# Replace NAs with empty strings\nsummary_stats[is.na(summary_stats)] &lt;- \" \"\n\ndatasummary_df(\n  summary_stats,\n  output = \"latex_tabular\"\n) %&gt;%\n  group_rows(\"Variables\", start_row = 1, end_row = nrow(variable_distributions), bold = F, italic = T) %&gt;%\n  group_rows(\"Counts\", start_row = nrow(variable_distributions) + 1, end_row = nrow(summary_stats), bold = F, italic = T) %&gt;%\n  write_lines(\"summary-stats.tex\")"
  },
  {
    "objectID": "posts/2024-04-06-tables-update/index.html#regression-estimates",
    "href": "posts/2024-04-06-tables-update/index.html#regression-estimates",
    "title": "Making LaTeX tables that look nice (2024 update)",
    "section": "Regression estimates",
    "text": "Regression estimates\nI’m a big fan of fixest for estimating regressions. It is extremely fast for models with lots of observations and fixed effects, and it includes a very nice set of functions for summarizing regression output. First we’ll estimate the models, then we’ll make the table.\n\nEstimate regression models\nMy tables often use similar sets of control variables with different sets of fixed effects. Content note: this is a regression annual automobile fatalities on a few variables, including the unemployment rate, income, beer taxes, and miles driven. The causal inference economist in me is required to disclose that I don’t think we’re estimating true causal effects here, but it’s a nice example for a table. I use use etable at the end of this chunk for a quick view of the estimates.\n\nfmla &lt;- \"fatal1000s ~ unemprate + income1000s + beertax + miles1000s\"\n\nmodels &lt;- list()\nmodels[[\"OLS\"]] &lt;- feols(as.formula(fmla), data = data)\nmodels[[\"+ State FE\"]] &lt;- feols(as.formula(sprintf(\"%s | state\", fmla)), data = data)\nmodels[[\"+ Year FE\"]] &lt;- feols(as.formula(sprintf(\"%s | state + year\", fmla)), data = data)\n\netable(models, cluster = ~state)\n\n                              OLS         + State FE          + Year FE\nDependent Var.:        fatal1000s         fatal1000s         fatal1000s\n                                                                       \nConstant          -3.401* (1.649)                                      \nunemprate       0.1371** (0.0483) -0.0179** (0.0066) -0.0322** (0.0105)\nincome1000s      0.2250* (0.0870)    0.0200 (0.0140)   0.0365. (0.0186)\nbeertax         0.0048** (0.0016)  -0.0025* (0.0012)  -0.0028. (0.0015)\nmiles1000s       -0.0060 (0.0507)   -0.0021 (0.0037)    0.0016 (0.0032)\nFixed-Effects:  ----------------- ------------------ ------------------\nstate                          No                Yes                Yes\nyear                           No                 No                Yes\n_______________ _________________ __________________ __________________\nS.E.: Clustered         by: state          by: state          by: state\nObservations                  336                336                336\nR2                        0.17631            0.99245            0.99326\nWithin R2                      --            0.26298            0.25553\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nTurn into LaTeX\nBut etable is capable of much more. Below I set options appropriate for LaTeX output.\n\nsetFixest_dict(var_dict)\n\netable(models,\n  cluster = ~state,\n  drop = \"Constant\",\n  digits = \"r3\",\n  digits.stats = 2,\n  fitstat = c(\"n\", \"r2\"),\n  style.tex = style.tex(\"aer\",\n    fixef.suffix = \" FEs\",\n    fixef.where = \"var\",\n    yesNo = c(\"Yes\", \"No\")\n  ),\n  tex = T\n) %&gt;%\n  write_lines(\"regression-etable.tex\")"
  },
  {
    "objectID": "posts/2024-04-06-tables-update/index.html#latex-document",
    "href": "posts/2024-04-06-tables-update/index.html#latex-document",
    "title": "Making LaTeX tables that look nice (2024 update)",
    "section": "LaTeX document",
    "text": "LaTeX document\nThat’s basically it. The last step is to plonk these into a LaTeX document (the output of which you saw above), copied below. It’s fairly simple, but I do use the threeparttable package to add notes to the tables.\n\\documentclass{article}\n\\usepackage{booktabs} \n\\usepackage{caption}\n\\usepackage[para]{threeparttable}\n\\usepackage{siunitx}\n\n\\begin{document}\n\n\\begin{table}\n\\centering\n\\begin{threeparttable}\n\\caption{Summary statistics}\n\\input{summary-stats}\n\\begin{tablenotes}\n \\item \\emph{Notes}: Table notes here.\n\\end{tablenotes}\n\\end{threeparttable}\n\\end{table}\n\n\\begin{table}\n\\centering\n\\begin{threeparttable}\n\\caption{Regression estimates}\n\\input{regression-etable}\n\\begin{tablenotes}\n \\item \\emph{Notes}: Table notes here.\n\\end{tablenotes}\n\\end{threeparttable}\n\\end{table}\n\n\\end{document}"
  },
  {
    "objectID": "posts/2024-04-06-tables-update/index.html#footnotes",
    "href": "posts/2024-04-06-tables-update/index.html#footnotes",
    "title": "Making LaTeX tables that look nice (2024 update)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn understatement. I’ve been very bad at updating it.↩︎"
  },
  {
    "objectID": "research.html#selected-work-in-progress",
    "href": "research.html#selected-work-in-progress",
    "title": "Research",
    "section": "Selected work in progress",
    "text": "Selected work in progress\n\nLabor Lock-in in the Fossil Fuel Industry (with Eva Lyubich and Katherine Wagner)\nThe Economic Consequences of Property Destruction (with Judson Boomhower, Jonathan Colmer and John Voorheis)\nBarriers to Civic Action on Clean Air (with Shweta Bhogale and Teevrat Garg)\nAdaptation and Insurance in the Agriculture Sector (with Talha Naeem, and Guglielmo Zappala)"
  }
]